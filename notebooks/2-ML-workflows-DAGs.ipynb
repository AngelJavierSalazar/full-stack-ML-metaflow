{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "923b9a08-9b88-424d-9512-4fef84a51b4f",
   "metadata": {},
   "source": [
    "## Writing Local Machine Learning Flows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f611f465-a272-434b-8331-2158cf8e9870",
   "metadata": {},
   "source": [
    "In this section, we take the machine learning scripts from the previous lesson and turn them into flows. Currently, in the spirit of not introducing more tools, we'll write our flows in notebook cells and execute them here also (this may change)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e696b923-37eb-4412-8d02-72105436a36d",
   "metadata": {},
   "source": [
    "### What is a (meta)flow?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7fb830-897a-4064-803a-eb58c6d40582",
   "metadata": {},
   "source": [
    "Include brief introduction to DAGs and use similar images to this (or find higher res version):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ba22da-ed85-4c78-9cea-fac7d704dbb8",
   "metadata": {},
   "source": [
    "![flow0](../img/flow_ex_0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f849a81d-20c8-4337-8050-b69f328cc9fe",
   "metadata": {},
   "source": [
    "Flows and DAGs can often be more complicated:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832a624d-6cd3-4dd7-b98a-d79439c9a7b7",
   "metadata": {},
   "source": [
    "![flow0](../img/flow_ex_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed0ab17-f71d-4e35-8862-346db0b1bbdb",
   "metadata": {},
   "source": [
    "So ML flows can be broken down into steps, such as:\n",
    "\n",
    "- importing data\n",
    "- processing, wrangling, and/or transforming the data\n",
    "- data validation\n",
    "- model configuration\n",
    "- model training, and\n",
    "- model deployment.\n",
    "\n",
    "The first flow we write will be a template showing these steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e25950f2-13e6-4f33-a882-8f928b952b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../flows/flow_template.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../flows/flow_template.py\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "Template for writing Metaflows\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from metaflow import FlowSpec, step, batch, current, environment, S3, card\n",
    "\n",
    "\n",
    "class Template_Flow(FlowSpec):\n",
    "    \"\"\"\n",
    "    Template for Metaflows.\n",
    "    You can choose which steps suit your workflow.\n",
    "    We have included the following common steps:\n",
    "    - Start\n",
    "    - Process data\n",
    "    - Data validation\n",
    "    - Model configuration\n",
    "    - Model training\n",
    "    - Model deployment\n",
    "    \"\"\"\n",
    "    \n",
    "    @card\n",
    "    @step\n",
    "    def start(self):\n",
    "        \"\"\"\n",
    "        Start Step for a Flow;\n",
    "        \"\"\"\n",
    "        print(\"flow name: %s\" % current.flow_name)\n",
    "        print(\"run id: %s\" % current.run_id)\n",
    "        print(\"username: %s\" % current.username)\n",
    "\n",
    "        # Call next step in DAG with self.next(...)\n",
    "        self.next(self.process_raw_data)\n",
    "\n",
    "    @step\n",
    "    def process_raw_data(self):\n",
    "        \"\"\"\n",
    "        Read and process data\n",
    "        \"\"\"\n",
    "        print(\"In this step, you'll read in and process your data\")\n",
    "\n",
    "        self.next(self.data_validation)\n",
    "\n",
    "    @step\n",
    "    def data_validation(self):\n",
    "        \"\"\"\n",
    "        Perform data validation\n",
    "        \"\"\"\n",
    "        print(\"In this step, you'll write your data validation code\")\n",
    "\n",
    "        self.next(self.get_model_config)\n",
    "\n",
    "    @step\n",
    "    def get_model_config(self):\n",
    "        \"\"\"\n",
    "        Configure model + hyperparams\n",
    "        \"\"\"\n",
    "        print(\"In this step, you'll configure your model + hyperparameters\")\n",
    "        self.next(self.train_model)\n",
    "\n",
    "    @step\n",
    "    def train_model(self):\n",
    "        \"\"\"\n",
    "        Train your model\n",
    "        \"\"\"\n",
    "        print(\"In this step, you'll train your model\")\n",
    "\n",
    "        self.next(self.deploy)\n",
    "\n",
    "    @step\n",
    "    def deploy(self):\n",
    "        \"\"\"\n",
    "        Deploy model\n",
    "        \"\"\"\n",
    "        print(\"In this step, you'll deploy your model\")\n",
    "\n",
    "        self.next(self.end)\n",
    "\n",
    "    @step\n",
    "    def end(self):\n",
    "        \"\"\"\n",
    "        DAG is done! Congrats!\n",
    "        \"\"\"\n",
    "        print('DAG ended! Woohoo!')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    Template_Flow()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8eff4783-4962-4422-8654-bb3d40abb1e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m\u001b[1mMetaflow 2.5.0\u001b[0m\u001b[35m\u001b[22m executing \u001b[0m\u001b[31m\u001b[1mTemplate_Flow\u001b[0m\u001b[35m\u001b[22m\u001b[0m\u001b[35m\u001b[22m for \u001b[0m\u001b[31m\u001b[1muser:hba\u001b[0m\u001b[35m\u001b[22m\u001b[K\u001b[0m\u001b[35m\u001b[22m\u001b[0m\n",
      "\u001b[35m\u001b[22mValidating your flow...\u001b[K\u001b[0m\u001b[35m\u001b[22m\u001b[0m\n",
      "\u001b[32m\u001b[1m    The graph looks good!\u001b[K\u001b[0m\u001b[32m\u001b[1m\u001b[0m\n",
      "\u001b[35m\u001b[22mRunning pylint...\u001b[K\u001b[0m\u001b[35m\u001b[22m\u001b[0m\n",
      "\u001b[32m\u001b[1m    Pylint is happy!\u001b[K\u001b[0m\u001b[32m\u001b[1m\u001b[0m\n",
      "\u001b[35m2022-03-16 18:14:46.153 \u001b[0m\u001b[1mWorkflow starting (run-id 7243):\u001b[0m\n",
      "\u001b[35m2022-03-16 18:14:51.950 \u001b[0m\u001b[32m[7243/start/135806 (pid 20275)] \u001b[0m\u001b[1mTask is starting.\u001b[0m\n",
      "\u001b[35m2022-03-16 18:14:59.011 \u001b[0m\u001b[32m[7243/start/135806 (pid 20275)] \u001b[0m\u001b[22mflow name: Template_Flow\u001b[0m\n",
      "\u001b[35m2022-03-16 18:15:26.364 \u001b[0m\u001b[32m[7243/start/135806 (pid 20275)] \u001b[0m\u001b[22mrun id: 7243\u001b[0m\n",
      "\u001b[35m2022-03-16 18:15:26.365 \u001b[0m\u001b[32m[7243/start/135806 (pid 20275)] \u001b[0m\u001b[22musername: hba\u001b[0m\n",
      "\u001b[35m2022-03-16 18:15:29.321 \u001b[0m\u001b[32m[7243/start/135806 (pid 20275)] \u001b[0m\u001b[1mTask finished successfully.\u001b[0m\n",
      "\u001b[35m2022-03-16 18:15:33.326 \u001b[0m\u001b[32m[7243/process_raw_data/135809 (pid 20311)] \u001b[0m\u001b[1mTask is starting.\u001b[0m\n",
      "\u001b[35m2022-03-16 18:15:40.474 \u001b[0m\u001b[32m[7243/process_raw_data/135809 (pid 20311)] \u001b[0m\u001b[22mIn this step, you'll read in and process your data\u001b[0m\n",
      "\u001b[35m2022-03-16 18:15:49.033 \u001b[0m\u001b[32m[7243/process_raw_data/135809 (pid 20311)] \u001b[0m\u001b[1mTask finished successfully.\u001b[0m\n",
      "\u001b[35m2022-03-16 18:15:52.938 \u001b[0m\u001b[32m[7243/data_validation/135810 (pid 20318)] \u001b[0m\u001b[1mTask is starting.\u001b[0m\n",
      "\u001b[35m2022-03-16 18:16:00.089 \u001b[0m\u001b[32m[7243/data_validation/135810 (pid 20318)] \u001b[0m\u001b[22mIn this step, you'll write your data validation code\u001b[0m\n",
      "\u001b[35m2022-03-16 18:16:08.666 \u001b[0m\u001b[32m[7243/data_validation/135810 (pid 20318)] \u001b[0m\u001b[1mTask finished successfully.\u001b[0m\n",
      "\u001b[35m2022-03-16 18:16:12.625 \u001b[0m\u001b[32m[7243/get_model_config/135811 (pid 20323)] \u001b[0m\u001b[1mTask is starting.\u001b[0m\n",
      "\u001b[35m2022-03-16 18:16:19.816 \u001b[0m\u001b[32m[7243/get_model_config/135811 (pid 20323)] \u001b[0m\u001b[22mIn this step, you'll configure your model + hyperparameters\u001b[0m\n",
      "\u001b[35m2022-03-16 18:16:28.869 \u001b[0m\u001b[32m[7243/get_model_config/135811 (pid 20323)] \u001b[0m\u001b[1mTask finished successfully.\u001b[0m\n",
      "\u001b[35m2022-03-16 18:16:32.751 \u001b[0m\u001b[32m[7243/train_model/135812 (pid 20336)] \u001b[0m\u001b[1mTask is starting.\u001b[0m\n",
      "\u001b[35m2022-03-16 18:16:40.121 \u001b[0m\u001b[32m[7243/train_model/135812 (pid 20336)] \u001b[0m\u001b[22mIn this step, you'll train your model\u001b[0m\n",
      "\u001b[35m2022-03-16 18:16:48.541 \u001b[0m\u001b[32m[7243/train_model/135812 (pid 20336)] \u001b[0m\u001b[1mTask finished successfully.\u001b[0m\n",
      "\u001b[35m2022-03-16 18:16:52.430 \u001b[0m\u001b[32m[7243/deploy/135813 (pid 20341)] \u001b[0m\u001b[1mTask is starting.\u001b[0m\n",
      "\u001b[35m2022-03-16 18:16:59.417 \u001b[0m\u001b[32m[7243/deploy/135813 (pid 20341)] \u001b[0m\u001b[22mIn this step, you'll deploy your model\u001b[0m\n",
      "\u001b[35m2022-03-16 18:17:08.123 \u001b[0m\u001b[32m[7243/deploy/135813 (pid 20341)] \u001b[0m\u001b[1mTask finished successfully.\u001b[0m\n",
      "\u001b[35m2022-03-16 18:17:12.217 \u001b[0m\u001b[32m[7243/end/135814 (pid 20346)] \u001b[0m\u001b[1mTask is starting.\u001b[0m\n",
      "\u001b[35m2022-03-16 18:17:19.353 \u001b[0m\u001b[32m[7243/end/135814 (pid 20346)] \u001b[0m\u001b[22mDAG ended! Woohoo!\u001b[0m\n",
      "\u001b[35m2022-03-16 18:17:27.829 \u001b[0m\u001b[32m[7243/end/135814 (pid 20346)] \u001b[0m\u001b[1mTask finished successfully.\u001b[0m\n",
      "\u001b[35m2022-03-16 18:17:28.981 \u001b[0m\u001b[1mDone!\u001b[0m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! python ../flows/flow_template.py run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727715f2-98e2-461f-9841-e019c3a66dba",
   "metadata": {},
   "source": [
    "What are all these outputs? I'm glad that you asked!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f0f3cb-4faf-452d-8a81-e457f01c286d",
   "metadata": {},
   "source": [
    "![flow0](../img/mf_output.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3594071a-8dbf-4e42-b488-f7d53cb54152",
   "metadata": {},
   "source": [
    "- **Timestamp** denotes when the line was output.\n",
    "- The information inside the square brackets identifies a **task**.\n",
    "- Every Metaflow run gets a unique ID, a **run ID**.\n",
    "- A run executes the steps in order. The step that is currently being executed is denoted by **step name**.\n",
    "- A step may spawn multiple tasks which are identified by a **task ID**.\n",
    "- The combination of a flow name, run ID, step name, and a task ID,uniquely identify a task in your Metaflow environment, amongst all runs of any flows. Here, the flow name is omitted since it is the same for all lines. We call this globally unique identifier a **pathspec**.\n",
    "- Each task is executed by a separate process in your operating system, identified by a **process ID** aka _pid_. You can use any operating system level monitoring tools such as top to monitor resource consumption of a task based on its process ID.\n",
    "- After the square bracket comes a **log message** that may be a message output by Metaflow itself, like “Task is starting” in this example, or a line output by your code.\n",
    "\n",
    "_Note_: The above bullets are almost verbatim from VT's book."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783703ae-daa1-4126-97cb-afcbc2511cd8",
   "metadata": {},
   "source": [
    "### Metaflow cards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b495bbe1-9619-4910-b693-365ebfb840e0",
   "metadata": {},
   "source": [
    "We can use MF cards to visualize aspects of our flow. In this case, there's not much to check out but we **can** see the DAG!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dadf0e77-0f7e-4aa0-a598-6aefff266f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m\u001b[1mMetaflow 2.5.0\u001b[0m\u001b[35m\u001b[22m executing \u001b[0m\u001b[31m\u001b[1mTemplate_Flow\u001b[0m\u001b[35m\u001b[22m\u001b[0m\u001b[35m\u001b[22m for \u001b[0m\u001b[31m\u001b[1muser:hba\u001b[0m\u001b[35m\u001b[22m\u001b[K\u001b[0m\u001b[35m\u001b[22m\u001b[0m\n",
      "\u001b[32m\u001b[22mResolving card: Template_Flow/7243/start/135806\u001b[K\u001b[0m\u001b[32m\u001b[22m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! python ../flows/flow_template.py card view start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b756922-edf6-44d9-8994-e4414ceb290a",
   "metadata": {},
   "source": [
    "### Random Forests ---> Metaflows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ebaf4f-7e2c-4c1c-aa62-9a045c9275e7",
   "metadata": {},
   "source": [
    "In this section, we'll turn the random forest above into a flow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5a864d3-32ad-4841-a684-66a53c3c4a95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../flows/local/rf_flow.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../flows/local/rf_flow.py\n",
    "\n",
    "from metaflow import FlowSpec, step, Parameter, JSONType, IncludeFile, card\n",
    "import json\n",
    "\n",
    "class ClassificationFlow(FlowSpec):\n",
    "    \"\"\"\n",
    "    train a random forest\n",
    "    \"\"\"\n",
    "    @card \n",
    "    @step\n",
    "    def start(self):\n",
    "        \"\"\"\n",
    "        Load the data\n",
    "        \"\"\"\n",
    "        #Import scikit-learn dataset library\n",
    "        from sklearn import datasets\n",
    "\n",
    "        #Load dataset\n",
    "        self.iris = datasets.load_iris()\n",
    "        self.X = self.iris['data']\n",
    "        self.y = self.iris['target']\n",
    "        self.next(self.rf_model)\n",
    "        \n",
    "\n",
    "    @step\n",
    "    def rf_model(self):\n",
    "        \"\"\"\n",
    "        build random forest model\n",
    "        \"\"\"\n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        \n",
    "        self.clf = RandomForestClassifier(n_estimators=10, max_depth=None,\n",
    "            min_samples_split=2, random_state=0)\n",
    "        self.next(self.train)\n",
    "\n",
    "        \n",
    "        \n",
    "    @step\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Train the model\n",
    "        \"\"\"\n",
    "        from sklearn.model_selection import cross_val_score\n",
    "        self.scores = cross_val_score(self.clf, self.X, self.y, cv=5)\n",
    "        self.next(self.end)\n",
    "        \n",
    "        \n",
    "    @step\n",
    "    def end(self):\n",
    "        \"\"\"\n",
    "        End of flow, yo!\n",
    "        \"\"\"\n",
    "        print(\"ClassificationFlow is all done.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ClassificationFlow()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7703623-d4cb-4641-81f1-19ddb43c6fb0",
   "metadata": {},
   "source": [
    "Execute the above from the command line with\n",
    "\n",
    "```bash\n",
    "python flows/local/rf_flow.py run\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37f0883b-a118-4aa6-a340-f0afde05c376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m\u001b[1mMetaflow 2.5.0\u001b[0m\u001b[35m\u001b[22m executing \u001b[0m\u001b[31m\u001b[1mClassificationFlow\u001b[0m\u001b[35m\u001b[22m\u001b[0m\u001b[35m\u001b[22m for \u001b[0m\u001b[31m\u001b[1muser:hba\u001b[0m\u001b[35m\u001b[22m\u001b[K\u001b[0m\u001b[35m\u001b[22m\u001b[0m\n",
      "\u001b[35m\u001b[22mValidating your flow...\u001b[K\u001b[0m\u001b[35m\u001b[22m\u001b[0m\n",
      "\u001b[32m\u001b[1m    The graph looks good!\u001b[K\u001b[0m\u001b[32m\u001b[1m\u001b[0m\n",
      "\u001b[35m\u001b[22mRunning pylint...\u001b[K\u001b[0m\u001b[35m\u001b[22m\u001b[0m\n",
      "\u001b[32m\u001b[1m    Pylint is happy!\u001b[K\u001b[0m\u001b[32m\u001b[1m\u001b[0m\n",
      "\u001b[35m2022-03-16 18:18:27.020 \u001b[0m\u001b[1mWorkflow starting (run-id 7245):\u001b[0m\n",
      "\u001b[35m2022-03-16 18:18:32.947 \u001b[0m\u001b[32m[7245/start/135816 (pid 20368)] \u001b[0m\u001b[1mTask is starting.\u001b[0m\n",
      "\u001b[35m2022-03-16 18:19:13.049 \u001b[0m\u001b[32m[7245/start/135816 (pid 20368)] \u001b[0m\u001b[1mTask finished successfully.\u001b[0m\n",
      "\u001b[35m2022-03-16 18:19:16.903 \u001b[0m\u001b[32m[7245/rf_model/135817 (pid 20381)] \u001b[0m\u001b[1mTask is starting.\u001b[0m\n",
      "\u001b[35m2022-03-16 18:19:33.403 \u001b[0m\u001b[32m[7245/rf_model/135817 (pid 20381)] \u001b[0m\u001b[1mTask finished successfully.\u001b[0m\n",
      "\u001b[35m2022-03-16 18:19:37.200 \u001b[0m\u001b[32m[7245/train/135818 (pid 20389)] \u001b[0m\u001b[1mTask is starting.\u001b[0m\n",
      "\u001b[35m2022-03-16 18:19:56.619 \u001b[0m\u001b[32m[7245/train/135818 (pid 20389)] \u001b[0m\u001b[1mTask finished successfully.\u001b[0m\n",
      "\u001b[35m2022-03-16 18:20:00.517 \u001b[0m\u001b[32m[7245/end/135819 (pid 20399)] \u001b[0m\u001b[1mTask is starting.\u001b[0m\n",
      "\u001b[35m2022-03-16 18:20:07.624 \u001b[0m\u001b[32m[7245/end/135819 (pid 20399)] \u001b[0m\u001b[22mClassificationFlow is all done.\u001b[0m\n",
      "\u001b[35m2022-03-16 18:20:16.141 \u001b[0m\u001b[32m[7245/end/135819 (pid 20399)] \u001b[0m\u001b[1mTask finished successfully.\u001b[0m\n",
      "\u001b[35m2022-03-16 18:20:17.333 \u001b[0m\u001b[1mDone!\u001b[0m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! python ../flows/local/rf_flow.py run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53459547-9b72-4eec-944e-71db6517faa0",
   "metadata": {},
   "source": [
    "We can check out the Metaflow card:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e89f934a-fc7f-4185-a61b-25f32dfa273c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m\u001b[1mMetaflow 2.5.0\u001b[0m\u001b[35m\u001b[22m executing \u001b[0m\u001b[31m\u001b[1mClassificationFlow\u001b[0m\u001b[35m\u001b[22m\u001b[0m\u001b[35m\u001b[22m for \u001b[0m\u001b[31m\u001b[1muser:hba\u001b[0m\u001b[35m\u001b[22m\u001b[K\u001b[0m\u001b[35m\u001b[22m\u001b[0m\n",
      "\u001b[32m\u001b[22mResolving card: ClassificationFlow/7245/start/135816\u001b[K\u001b[0m\u001b[32m\u001b[22m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! python ../flows/local/rf_flow.py card view start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde03f82-6d44-476c-99b2-1eb41e390d4f",
   "metadata": {},
   "source": [
    "Now we'll write a flow that has random forests, decision trees, and extra trees classifiers, tries them all and chooses the best one. We'll use the concept of branching, which is exemplified in this figure:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927f3fcf-3f11-4885-a68b-87adc23c016d",
   "metadata": {},
   "source": [
    "![flow0](../img/flow_ex_0.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5545fc2d-dcc4-4a37-bc8b-37ed28f98ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../flows/local/tree_branch_flow.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../flows/local/tree_branch_flow.py\n",
    "\n",
    "from metaflow import FlowSpec, step, Parameter, JSONType, IncludeFile, card\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ClassificationFlow(FlowSpec):\n",
    "    \"\"\"\n",
    "    train multiple tree based methods\n",
    "    \"\"\"\n",
    "    @card \n",
    "    @step\n",
    "    def start(self):\n",
    "        \"\"\"\n",
    "        Load the data\n",
    "        \"\"\"\n",
    "        #Import scikit-learn dataset library\n",
    "        from sklearn import datasets\n",
    "\n",
    "        #Load dataset\n",
    "        self.iris = datasets.load_iris()\n",
    "        self.X = self.iris['data']\n",
    "        self.y = self.iris['target']\n",
    "        self.next(self.rf_model, self.xt_model, self.dt_model)\n",
    "    \n",
    "                \n",
    "    @step\n",
    "    def rf_model(self):\n",
    "        \"\"\"\n",
    "        build random forest model\n",
    "        \"\"\"\n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        from sklearn.model_selection import cross_val_score\n",
    "        \n",
    "        self.clf = RandomForestClassifier(n_estimators=10, max_depth=None,\n",
    "            min_samples_split=2, random_state=0)\n",
    "        self.scores = cross_val_score(self.clf, self.X, self.y, cv=5)\n",
    "        self.next(self.choose_model)\n",
    "\n",
    "    @step\n",
    "    def xt_model(self):\n",
    "        \"\"\"\n",
    "        build extra trees classifier\n",
    "        \"\"\"\n",
    "        from sklearn.ensemble import ExtraTreesClassifier\n",
    "        from sklearn.model_selection import cross_val_score\n",
    "        \n",
    "\n",
    "        self.clf = ExtraTreesClassifier(n_estimators=10, max_depth=None,\n",
    "            min_samples_split=2, random_state=0)\n",
    "\n",
    "        self.scores = cross_val_score(self.clf, self.X, self.y, cv=5)\n",
    "        self.next(self.choose_model)\n",
    "\n",
    "    @step\n",
    "    def dt_model(self):\n",
    "        \"\"\"\n",
    "        build decision tree classifier\n",
    "        \"\"\"\n",
    "        from sklearn.tree import DecisionTreeClassifier\n",
    "        from sklearn.model_selection import cross_val_score\n",
    "        \n",
    "        self.clf = DecisionTreeClassifier(max_depth=None, min_samples_split=2,\n",
    "            random_state=0)\n",
    "\n",
    "        self.scores = cross_val_score(self.clf, self.X, self.y, cv=5)\n",
    "\n",
    "        self.next(self.choose_model)\n",
    "                        \n",
    "    @step\n",
    "    def choose_model(self, inputs):\n",
    "        \"\"\"\n",
    "        find 'best' model\n",
    "        \"\"\"\n",
    "        import numpy as np\n",
    "\n",
    "        def score(inp):\n",
    "            return inp.clf,\\\n",
    "                   np.mean(inp.scores)\n",
    "\n",
    "            \n",
    "        self.results = sorted(map(score, inputs), key=lambda x: -x[1]) \n",
    "        self.model = self.results[0][0]\n",
    "        self.next(self.end)\n",
    "        \n",
    "    @step\n",
    "    def end(self):\n",
    "        \"\"\"\n",
    "        End of flow, yo!\n",
    "        \"\"\"\n",
    "        print('Scores:')\n",
    "        print('\\n'.join('%s %f' % res for res in self.results))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ClassificationFlow()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6fc354-9ef2-4326-a913-f2299fd3d8b4",
   "metadata": {},
   "source": [
    "Execute the above from the command line with\n",
    "\n",
    "```bash\n",
    "python flows/local/tree_branch_flow.py run\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3aa5a415-63b2-412f-a625-c7ccaeb30f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m\u001b[1mMetaflow 2.5.0\u001b[0m\u001b[35m\u001b[22m executing \u001b[0m\u001b[31m\u001b[1mClassificationFlow\u001b[0m\u001b[35m\u001b[22m\u001b[0m\u001b[35m\u001b[22m for \u001b[0m\u001b[31m\u001b[1muser:hba\u001b[0m\u001b[35m\u001b[22m\u001b[K\u001b[0m\u001b[35m\u001b[22m\u001b[0m\n",
      "\u001b[35m\u001b[22mValidating your flow...\u001b[K\u001b[0m\u001b[35m\u001b[22m\u001b[0m\n",
      "\u001b[32m\u001b[1m    The graph looks good!\u001b[K\u001b[0m\u001b[32m\u001b[1m\u001b[0m\n",
      "\u001b[35m\u001b[22mRunning pylint...\u001b[K\u001b[0m\u001b[35m\u001b[22m\u001b[0m\n",
      "\u001b[32m\u001b[1m    Pylint is happy!\u001b[K\u001b[0m\u001b[32m\u001b[1m\u001b[0m\n",
      "\u001b[35m2022-03-16 18:20:59.364 \u001b[0m\u001b[1mWorkflow starting (run-id 7246):\u001b[0m\n",
      "\u001b[35m2022-03-16 18:21:05.388 \u001b[0m\u001b[32m[7246/start/135822 (pid 20422)] \u001b[0m\u001b[1mTask is starting.\u001b[0m\n",
      "\u001b[35m2022-03-16 18:21:51.824 \u001b[0m\u001b[32m[7246/start/135822 (pid 20422)] \u001b[0m\u001b[1mTask finished successfully.\u001b[0m\n",
      "\u001b[35m2022-03-16 18:21:55.776 \u001b[0m\u001b[32m[7246/rf_model/135823 (pid 20435)] \u001b[0m\u001b[1mTask is starting.\u001b[0m\n",
      "\u001b[35m2022-03-16 18:21:58.623 \u001b[0m\u001b[32m[7246/xt_model/135824 (pid 20439)] \u001b[0m\u001b[1mTask is starting.\u001b[0m\n",
      "\u001b[35m2022-03-16 18:22:01.514 \u001b[0m\u001b[32m[7246/dt_model/135825 (pid 20443)] \u001b[0m\u001b[1mTask is starting.\u001b[0m\n",
      "\u001b[35m2022-03-16 18:22:13.882 \u001b[0m\u001b[32m[7246/rf_model/135823 (pid 20435)] \u001b[0m\u001b[1mTask finished successfully.\u001b[0m\n",
      "\u001b[35m2022-03-16 18:22:17.445 \u001b[0m\u001b[32m[7246/xt_model/135824 (pid 20439)] \u001b[0m\u001b[1mTask finished successfully.\u001b[0m\n",
      "\u001b[35m2022-03-16 18:22:21.087 \u001b[0m\u001b[32m[7246/dt_model/135825 (pid 20443)] \u001b[0m\u001b[1mTask finished successfully.\u001b[0m\n",
      "\u001b[35m2022-03-16 18:22:25.303 \u001b[0m\u001b[32m[7246/choose_model/135827 (pid 20461)] \u001b[0m\u001b[1mTask is starting.\u001b[0m\n",
      "\u001b[35m2022-03-16 18:22:49.693 \u001b[0m\u001b[32m[7246/choose_model/135827 (pid 20461)] \u001b[0m\u001b[1mTask finished successfully.\u001b[0m\n",
      "\u001b[35m2022-03-16 18:22:53.528 \u001b[0m\u001b[32m[7246/end/135828 (pid 20469)] \u001b[0m\u001b[1mTask is starting.\u001b[0m\n",
      "\u001b[35m2022-03-16 18:23:00.462 \u001b[0m\u001b[32m[7246/end/135828 (pid 20469)] \u001b[0m\u001b[22mScores:\u001b[0m\n",
      "\u001b[35m2022-03-16 18:23:01.070 \u001b[0m\u001b[32m[7246/end/135828 (pid 20469)] \u001b[0m\u001b[22mDecisionTreeClassifier(random_state=0) 0.960000\u001b[0m\n",
      "\u001b[35m2022-03-16 18:23:06.972 \u001b[0m\u001b[32m[7246/end/135828 (pid 20469)] \u001b[0m\u001b[22mRandomForestClassifier(n_estimators=10, random_state=0) 0.953333\u001b[0m\n",
      "\u001b[35m2022-03-16 18:23:06.972 \u001b[0m\u001b[32m[7246/end/135828 (pid 20469)] \u001b[0m\u001b[22mExtraTreesClassifier(n_estimators=10, random_state=0) 0.953333\u001b[0m\n",
      "\u001b[35m2022-03-16 18:23:09.931 \u001b[0m\u001b[32m[7246/end/135828 (pid 20469)] \u001b[0m\u001b[1mTask finished successfully.\u001b[0m\n",
      "\u001b[35m2022-03-16 18:23:11.128 \u001b[0m\u001b[1mDone!\u001b[0m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! python ../flows/local/tree_branch_flow.py run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3498134b-2f69-4dd2-b604-6e9c6e44fd49",
   "metadata": {},
   "source": [
    "We can also view the Metaflow card:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a321df46-1ba8-4035-add6-a81d5ebe23d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m\u001b[1mMetaflow 2.5.0\u001b[0m\u001b[35m\u001b[22m executing \u001b[0m\u001b[31m\u001b[1mClassificationFlow\u001b[0m\u001b[35m\u001b[22m\u001b[0m\u001b[35m\u001b[22m for \u001b[0m\u001b[31m\u001b[1muser:hba\u001b[0m\u001b[35m\u001b[22m\u001b[K\u001b[0m\u001b[35m\u001b[22m\u001b[0m\n",
      "\u001b[32m\u001b[22mResolving card: ClassificationFlow/7246/start/135822\u001b[K\u001b[0m\u001b[32m\u001b[22m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! python ../flows/local/tree_branch_flow.py card view start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2dd4a43-3ea3-43f5-9d2f-c96ee7d18bb3",
   "metadata": {},
   "source": [
    "### Boosted Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37b27a2-f26c-471d-8164-598ed7d4cfb0",
   "metadata": {},
   "source": [
    "In this section, we'll turn the xgboost example above into a flow.\n",
    "\n",
    "_Note:_ you've done this _almost_ trivially below, really to show yourself that you can get xgboost working with MF, HBA. Make a slightly stronger example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e0b86aa-498d-474f-bde4-1e183b5dd7a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../flows/local/boosted_flow.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../flows/local/boosted_flow.py\n",
    "\n",
    "from metaflow import FlowSpec, step, Parameter, JSONType, IncludeFile, card\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class BSTFlow(FlowSpec):\n",
    "    \"\"\"\n",
    "    train a boosted tree\n",
    "    \"\"\"\n",
    "    @card\n",
    "    @step\n",
    "    def start(self):\n",
    "        \"\"\"\n",
    "        Load the data & train model\n",
    "        \"\"\"\n",
    "        import xgboost as xgb\n",
    "        # from io import StringIO\n",
    "        # read in data\n",
    "        dtrain = xgb.DMatrix('../data/agaricus.txt.train')\n",
    "        #dtest = xgb.DMatrix('data/agaricus.txt.test')\n",
    "\n",
    "                # specify parameters\n",
    "        param = {'max_depth':2, 'eta':1, 'objective':'binary:logistic' }\n",
    "        num_round = 2\n",
    "        bst = xgb.train(param, dtrain, num_round)\n",
    "        bst.save_model(\"model.json\")\n",
    "        self.next(self.predict)\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "    @step\n",
    "    def predict(self):\n",
    "        \"\"\"\n",
    "        make predictions\n",
    "        \"\"\"\n",
    "        import xgboost as xgb\n",
    "\n",
    "        dtest = xgb.DMatrix('../data/agaricus.txt.test')\n",
    "        # make prediction\n",
    "        bst = xgb.Booster()\n",
    "        bst.load_model(\"model.json\")\n",
    "        preds = bst.predict(dtest)\n",
    "        self.next(self.end)\n",
    "        \n",
    "        \n",
    "    @step\n",
    "    def end(self):\n",
    "        \"\"\"\n",
    "        End of flow, yo!\n",
    "        \"\"\"\n",
    "        print(\"ClassificationFlow is all done.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    BSTFlow()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a1fce4-47cd-4c23-b770-7af10f785aee",
   "metadata": {},
   "source": [
    "Execute the above from the command line with\n",
    "\n",
    "```bash\n",
    "python flows/local/boosted_flow.py run\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e817331-98cf-4eca-bea4-036fc41965c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m\u001b[1mMetaflow 2.5.0\u001b[0m\u001b[35m\u001b[22m executing \u001b[0m\u001b[31m\u001b[1mBSTFlow\u001b[0m\u001b[35m\u001b[22m\u001b[0m\u001b[35m\u001b[22m for \u001b[0m\u001b[31m\u001b[1muser:hba\u001b[0m\u001b[35m\u001b[22m\u001b[K\u001b[0m\u001b[35m\u001b[22m\u001b[0m\n",
      "\u001b[35m\u001b[22mValidating your flow...\u001b[K\u001b[0m\u001b[35m\u001b[22m\u001b[0m\n",
      "\u001b[32m\u001b[1m    The graph looks good!\u001b[K\u001b[0m\u001b[32m\u001b[1m\u001b[0m\n",
      "\u001b[35m\u001b[22mRunning pylint...\u001b[K\u001b[0m\u001b[35m\u001b[22m\u001b[0m\n",
      "\u001b[32m\u001b[1m    Pylint is happy!\u001b[K\u001b[0m\u001b[32m\u001b[1m\u001b[0m\n",
      "\u001b[35m2022-03-16 18:23:52.286 \u001b[0m\u001b[1mWorkflow starting (run-id 7247):\u001b[0m\n",
      "\u001b[35m2022-03-16 18:23:58.121 \u001b[0m\u001b[32m[7247/start/135831 (pid 20503)] \u001b[0m\u001b[1mTask is starting.\u001b[0m\n",
      "\u001b[35m2022-03-16 18:24:05.705 \u001b[0m\u001b[32m[7247/start/135831 (pid 20503)] \u001b[0m\u001b[22m[18:24:05] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\u001b[0m\n",
      "\u001b[35m2022-03-16 18:24:36.269 \u001b[0m\u001b[32m[7247/start/135831 (pid 20503)] \u001b[0m\u001b[1mTask finished successfully.\u001b[0m\n",
      "\u001b[35m2022-03-16 18:24:40.303 \u001b[0m\u001b[32m[7247/predict/135832 (pid 20521)] \u001b[0m\u001b[1mTask is starting.\u001b[0m\n",
      "\u001b[35m2022-03-16 18:24:56.877 \u001b[0m\u001b[32m[7247/predict/135832 (pid 20521)] \u001b[0m\u001b[1mTask finished successfully.\u001b[0m\n",
      "\u001b[35m2022-03-16 18:25:00.689 \u001b[0m\u001b[32m[7247/end/135833 (pid 20534)] \u001b[0m\u001b[1mTask is starting.\u001b[0m\n",
      "\u001b[35m2022-03-16 18:25:07.641 \u001b[0m\u001b[32m[7247/end/135833 (pid 20534)] \u001b[0m\u001b[22mClassificationFlow is all done.\u001b[0m\n",
      "\u001b[35m2022-03-16 18:25:16.180 \u001b[0m\u001b[32m[7247/end/135833 (pid 20534)] \u001b[0m\u001b[1mTask finished successfully.\u001b[0m\n",
      "\u001b[35m2022-03-16 18:25:17.287 \u001b[0m\u001b[1mDone!\u001b[0m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! python ../flows/local/boosted_flow.py run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074ee36d-c8d1-4d99-9f09-999324974ccd",
   "metadata": {},
   "source": [
    "We can also view the Metaflow card:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e4b1d4a9-ad3e-4043-a011-6229b92e1fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m\u001b[1mMetaflow 2.5.0\u001b[0m\u001b[35m\u001b[22m executing \u001b[0m\u001b[31m\u001b[1mBSTFlow\u001b[0m\u001b[35m\u001b[22m\u001b[0m\u001b[35m\u001b[22m for \u001b[0m\u001b[31m\u001b[1muser:hba\u001b[0m\u001b[35m\u001b[22m\u001b[K\u001b[0m\u001b[35m\u001b[22m\u001b[0m\n",
      "\u001b[32m\u001b[22mResolving card: BSTFlow/7247/start/135831\u001b[K\u001b[0m\u001b[32m\u001b[22m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! python ../flows/local/boosted_flow.py card view start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f757332f-b475-4e42-b546-c97a75068fc9",
   "metadata": {},
   "source": [
    "### Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e95df17-f187-4d4f-ad2e-894898f9b9b0",
   "metadata": {},
   "source": [
    "In this section, we'll turn the deep learning example above into a flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "92adeacf-5396-42d0-bed2-32a8a9763d1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../flows/local/NN_flow.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../flows/local/NN_flow.py\n",
    "\n",
    "from metaflow import FlowSpec, step, Parameter, JSONType, IncludeFile, card\n",
    "from taxi_modules import init, MODELS, MODEL_LIBRARIES\n",
    "import json\n",
    "\n",
    "\n",
    "class NNFlow(FlowSpec):\n",
    "    \"\"\"\n",
    "    train a NN\n",
    "    \"\"\"\n",
    "    @card\n",
    "    @step\n",
    "    def start(self):\n",
    "        \"\"\"\n",
    "        Load the data\n",
    "        \"\"\"\n",
    "        from tensorflow import keras\n",
    "\n",
    "        # the data, split between train and test sets\n",
    "        (self.x_train, self.y_train), (self.x_test, self.y_test) = keras.datasets.mnist.load_data()\n",
    "        self.next(self.wrangle)\n",
    "        \n",
    "    @step\n",
    "    def wrangle(self):\n",
    "        \"\"\"\n",
    "        massage data\n",
    "        \"\"\"\n",
    "        import numpy as np\n",
    "        from tensorflow import keras\n",
    "        # Model / data parameters\n",
    "        self.num_classes = 10\n",
    "        self.input_shape = (28, 28, 1)\n",
    "\n",
    "        # Scale images to the [0, 1] range\n",
    "        self.x_train = self.x_train.astype(\"float32\") / 255\n",
    "        self.x_test = self.x_test.astype(\"float32\") / 255\n",
    "        # Make sure images have shape (28, 28, 1)\n",
    "        self.x_train = np.expand_dims(self.x_train, -1)\n",
    "        self.x_test = np.expand_dims(self.x_test, -1)\n",
    "\n",
    "        # convert class vectors to binary class matrices\n",
    "        self.y_train = keras.utils.to_categorical(self.y_train, self.num_classes)\n",
    "        self.y_test = keras.utils.to_categorical(self.y_test, self.num_classes)\n",
    "        \n",
    "        self.next(self.build_model)\n",
    "\n",
    "\n",
    "    @step\n",
    "    def build_model(self):\n",
    "        \"\"\"\n",
    "        build NN model\n",
    "        \"\"\"\n",
    "        import tempfile\n",
    "        import numpy as np\n",
    "        import tensorflow as tf\n",
    "        from tensorflow import keras\n",
    "        from tensorflow.keras import layers\n",
    "\n",
    "        model = keras.Sequential(\n",
    "            [\n",
    "                keras.Input(shape=self.input_shape),\n",
    "                layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "                layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "                layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "                layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "                layers.Flatten(),\n",
    "                layers.Dropout(0.5),\n",
    "                layers.Dense(self.num_classes, activation=\"softmax\"),\n",
    "            ]\n",
    "        )\n",
    "        model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "        with tempfile.NamedTemporaryFile() as f:\n",
    "            tf.keras.models.save_model(model, f.name, save_format='h5')\n",
    "            self.model = f.read()\n",
    "        self.next(self.train)\n",
    "\n",
    "        \n",
    "        \n",
    "    @step\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Train the model\n",
    "        \"\"\"\n",
    "        import tempfile\n",
    "        import tensorflow as tf\n",
    "        self.batch_size = 128\n",
    "        self.epochs = 15\n",
    "        \n",
    "        with tempfile.NamedTemporaryFile() as f:\n",
    "            f.write(self.model)\n",
    "            f.flush()\n",
    "            model =  tf.keras.models.load_model(f.name)\n",
    "        model.fit(self.x_train, self.y_train, batch_size=self.batch_size, epochs=self.epochs, validation_split=0.1)\n",
    "        \n",
    "        self.next(self.end)\n",
    "        \n",
    "        \n",
    "    @step\n",
    "    def end(self):\n",
    "        \"\"\"\n",
    "        End of flow, yo!\n",
    "        \"\"\"\n",
    "        print(\"ClassificationFlow is all done.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    NNFlow()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bf1b5f-d6b9-4b89-be19-b3d22a5db0bd",
   "metadata": {},
   "source": [
    "Execute the above from the command line with\n",
    "\n",
    "```bash\n",
    "python flows/local/NN_flow.py run\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6aa50054-916d-4b5c-8f5b-4159996d95a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m\u001b[1mMetaflow 2.5.0\u001b[0m\u001b[35m\u001b[22m executing \u001b[0m\u001b[31m\u001b[1mNNFlow\u001b[0m\u001b[35m\u001b[22m\u001b[0m\u001b[35m\u001b[22m for \u001b[0m\u001b[31m\u001b[1muser:hba\u001b[0m\u001b[35m\u001b[22m\u001b[K\u001b[0m\u001b[35m\u001b[22m\u001b[0m\n",
      "\u001b[35m\u001b[22mValidating your flow...\u001b[K\u001b[0m\u001b[35m\u001b[22m\u001b[0m\n",
      "\u001b[32m\u001b[1m    The graph looks good!\u001b[K\u001b[0m\u001b[32m\u001b[1m\u001b[0m\n",
      "\u001b[35m\u001b[22mRunning pylint...\u001b[K\u001b[0m\u001b[35m\u001b[22m\u001b[0m\n",
      "\u001b[32m\u001b[1m    Pylint is happy!\u001b[K\u001b[0m\u001b[32m\u001b[1m\u001b[0m\n",
      "\u001b[35m2022-03-16 18:26:11.106 \u001b[0m\u001b[1mWorkflow starting (run-id 7249):\u001b[0m\n",
      "\u001b[35m2022-03-16 18:26:17.055 \u001b[0m\u001b[32m[7249/start/135837 (pid 20559)] \u001b[0m\u001b[1mTask is starting.\u001b[0m\n",
      "\u001b[35m2022-03-16 18:26:59.509 \u001b[0m\u001b[32m[7249/start/135837 (pid 20559)] \u001b[0m\u001b[1mTask finished successfully.\u001b[0m\n",
      "\u001b[35m2022-03-16 18:27:03.510 \u001b[0m\u001b[32m[7249/wrangle/135841 (pid 20585)] \u001b[0m\u001b[1mTask is starting.\u001b[0m\n",
      "\u001b[35m2022-03-16 18:27:30.330 \u001b[0m\u001b[32m[7249/wrangle/135841 (pid 20585)] \u001b[0m\u001b[1mTask finished successfully.\u001b[0m\n",
      "\u001b[35m2022-03-16 18:27:34.178 \u001b[0m\u001b[32m[7249/build_model/135842 (pid 20592)] \u001b[0m\u001b[1mTask is starting.\u001b[0m\n",
      "\u001b[35m2022-03-16 18:27:43.393 \u001b[0m\u001b[32m[7249/build_model/135842 (pid 20592)] \u001b[0m\u001b[22m2022-03-16 18:27:43.392804: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\u001b[0m\n",
      "\u001b[35m2022-03-16 18:27:54.924 \u001b[0m\u001b[32m[7249/build_model/135842 (pid 20592)] \u001b[0m\u001b[22mTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\u001b[0m\n",
      "\u001b[35m2022-03-16 18:27:57.927 \u001b[0m\u001b[32m[7249/build_model/135842 (pid 20592)] \u001b[0m\u001b[1mTask finished successfully.\u001b[0m\n",
      "\u001b[35m2022-03-16 18:28:02.238 \u001b[0m\u001b[32m[7249/train/135843 (pid 20604)] \u001b[0m\u001b[1mTask is starting.\u001b[0m\n",
      "\u001b[35m2022-03-16 18:28:11.336 \u001b[0m\u001b[32m[7249/train/135843 (pid 20604)] \u001b[0m\u001b[22m2022-03-16 18:28:11.336163: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\u001b[0m\n",
      "\u001b[35m2022-03-16 18:28:19.213 \u001b[0m\u001b[32m[7249/train/135843 (pid 20604)] \u001b[0m\u001b[22mTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\u001b[0m\n",
      "\u001b[35m2022-03-16 18:28:19.213 \u001b[0m\u001b[32m[7249/train/135843 (pid 20604)] \u001b[0m\u001b[22m2022-03-16 18:28:19.213023: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\u001b[0m\n",
      "\u001b[35m2022-03-16 18:28:19.223 \u001b[0m\u001b[32m[7249/train/135843 (pid 20604)] \u001b[0m\u001b[22mEpoch 1/15\u001b[0m\n",
      "422/422 [==============================] - 11s 26ms/step - loss: 0.3707 - accuracy: 0.8873 - val_loss: 0.0826 - val_accuracy: 0.9780\u001b[0m: 2.2740 - accuracy: 0.13\n",
      "\u001b[35m2022-03-16 18:28:30.227 \u001b[0m\u001b[32m[7249/train/135843 (pid 20604)] \u001b[0m\u001b[22mEpoch 2/15\u001b[0m\n",
      "422/422 [==============================] - 11s 25ms/step - loss: 0.1126 - accuracy: 0.9648 - val_loss: 0.0554 - val_accuracy: 0.9850\u001b[0m0.0576 - accuracy: 0.97\n",
      "\u001b[35m2022-03-16 18:28:40.730 \u001b[0m\u001b[32m[7249/train/135843 (pid 20604)] \u001b[0m\u001b[22mEpoch 3/15\u001b[0m\n",
      "422/422 [==============================] - 10s 25ms/step - loss: 0.0825 - accuracy: 0.9744 - val_loss: 0.0468 - val_accuracy: 0.9877\u001b[0m 0.1134 - accuracy: 0.960\n",
      "\u001b[35m2022-03-16 18:28:51.207 \u001b[0m\u001b[32m[7249/train/135843 (pid 20604)] \u001b[0m\u001b[22mEpoch 4/15\u001b[0m\n",
      "422/422 [==============================] - 11s 25ms/step - loss: 0.0701 - accuracy: 0.9785 - val_loss: 0.0457 - val_accuracy: 0.9873\u001b[0m0.1139 - accuracy: 0.9\n",
      "\u001b[35m2022-03-16 18:29:01.918 \u001b[0m\u001b[32m[7249/train/135843 (pid 20604)] \u001b[0m\u001b[22mEpoch 5/15\u001b[0m\n",
      "422/422 [==============================] - 11s 25ms/step - loss: 0.0622 - accuracy: 0.9803 - val_loss: 0.0370 - val_accuracy: 0.9893\u001b[0m 0.0518 - accuracy: 0.976\n",
      "\u001b[35m2022-03-16 18:29:12.462 \u001b[0m\u001b[32m[7249/train/135843 (pid 20604)] \u001b[0m\u001b[22mEpoch 6/15\u001b[0m\n",
      "422/422 [==============================] - 10s 25ms/step - loss: 0.0549 - accuracy: 0.9825 - val_loss: 0.0383 - val_accuracy: 0.9893\u001b[0m0.0250 - accuracy: 0.99\n",
      "\u001b[35m2022-03-16 18:29:22.850 \u001b[0m\u001b[32m[7249/train/135843 (pid 20604)] \u001b[0m\u001b[22mEpoch 7/15\u001b[0m\n",
      "422/422 [==============================] - 11s 25ms/step - loss: 0.0507 - accuracy: 0.9847 - val_loss: 0.0375 - val_accuracy: 0.9898\u001b[0m 0.0386 - accuracy: 0.98\n",
      "\u001b[35m2022-03-16 18:29:33.372 \u001b[0m\u001b[32m[7249/train/135843 (pid 20604)] \u001b[0m\u001b[22mEpoch 8/15\u001b[0m\n",
      "422/422 [==============================] - 10s 25ms/step - loss: 0.0465 - accuracy: 0.9859 - val_loss: 0.0340 - val_accuracy: 0.9903\u001b[0m 0.0332 - accuracy: 0.992\n",
      "\u001b[35m2022-03-16 18:29:43.770 \u001b[0m\u001b[32m[7249/train/135843 (pid 20604)] \u001b[0m\u001b[22mEpoch 9/15\u001b[0m\n",
      "422/422 [==============================] - 10s 25ms/step - loss: 0.0429 - accuracy: 0.9865 - val_loss: 0.0320 - val_accuracy: 0.9928\u001b[0m 0.0663 - accuracy: 0.984\n",
      "\u001b[35m2022-03-16 18:29:54.249 \u001b[0m\u001b[32m[7249/train/135843 (pid 20604)] \u001b[0m\u001b[22mEpoch 10/15\u001b[0m\n",
      "422/422 [==============================] - 11s 25ms/step - loss: 0.0406 - accuracy: 0.9867 - val_loss: 0.0315 - val_accuracy: 0.9917\u001b[0m0.0296 - accuracy: 0.99\n",
      "\u001b[35m2022-03-16 18:30:04.950 \u001b[0m\u001b[32m[7249/train/135843 (pid 20604)] \u001b[0m\u001b[22mEpoch 11/15\u001b[0m\n",
      "422/422 [==============================] - 11s 25ms/step - loss: 0.0392 - accuracy: 0.9881 - val_loss: 0.0327 - val_accuracy: 0.9913\u001b[0m0.0145 - accuracy: 1.00\n",
      "\u001b[35m2022-03-16 18:30:15.477 \u001b[0m\u001b[32m[7249/train/135843 (pid 20604)] \u001b[0m\u001b[22mEpoch 12/15\u001b[0m\n",
      "422/422 [==============================] - 10s 25ms/step - loss: 0.0365 - accuracy: 0.9882 - val_loss: 0.0331 - val_accuracy: 0.9915\u001b[0m0.0569 - accuracy: 0.98.] - ETA: 4s - loss: 0.0341 - accuracy: 0.98\n",
      "\u001b[35m2022-03-16 18:30:25.904 \u001b[0m\u001b[32m[7249/train/135843 (pid 20604)] \u001b[0m\u001b[22mEpoch 13/15\u001b[0m\n",
      "422/422 [==============================] - 10s 25ms/step - loss: 0.0352 - accuracy: 0.9884 - val_loss: 0.0298 - val_accuracy: 0.9918\u001b[0m 0.0078 - accuracy: 1.000\n",
      "\u001b[35m2022-03-16 18:30:36.280 \u001b[0m\u001b[32m[7249/train/135843 (pid 20604)] \u001b[0m\u001b[22mEpoch 14/15\u001b[0m\n",
      "422/422 [==============================] - 10s 25ms/step - loss: 0.0340 - accuracy: 0.9885 - val_loss: 0.0303 - val_accuracy: 0.9915\u001b[0m0.0265 - accuracy: 0.9\n",
      "\u001b[35m2022-03-16 18:30:46.721 \u001b[0m\u001b[32m[7249/train/135843 (pid 20604)] \u001b[0m\u001b[22mEpoch 15/15\u001b[0m\n",
      "422/422 [==============================] - 10s 25ms/step - loss: 0.0321 - accuracy: 0.9895 - val_loss: 0.0292 - val_accuracy: 0.9927\u001b[0m 0.0081 - accuracy: 1.000\n",
      "\u001b[35m2022-03-16 18:31:09.587 \u001b[0m\u001b[32m[7249/train/135843 (pid 20604)] \u001b[0m\u001b[1mTask finished successfully.\u001b[0m\n",
      "\u001b[35m2022-03-16 18:31:16.569 \u001b[0m\u001b[32m[7249/end/135845 (pid 20624)] \u001b[0m\u001b[1mTask is starting.\u001b[0m\n",
      "\u001b[35m2022-03-16 18:31:23.903 \u001b[0m\u001b[32m[7249/end/135845 (pid 20624)] \u001b[0m\u001b[22mClassificationFlow is all done.\u001b[0m\n",
      "\u001b[35m2022-03-16 18:31:32.768 \u001b[0m\u001b[32m[7249/end/135845 (pid 20624)] \u001b[0m\u001b[1mTask finished successfully.\u001b[0m\n",
      "\u001b[35m2022-03-16 18:31:34.033 \u001b[0m\u001b[1mDone!\u001b[0m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! python ../flows/local/NN_flow.py run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9d4468-2e0b-4110-9cad-c7ee2ce1a55c",
   "metadata": {},
   "source": [
    "We can also view the Metaflow card:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "676c337b-d846-4cf9-96ee-038f9eada440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m\u001b[1mMetaflow 2.5.0\u001b[0m\u001b[35m\u001b[22m executing \u001b[0m\u001b[31m\u001b[1mNNFlow\u001b[0m\u001b[35m\u001b[22m\u001b[0m\u001b[35m\u001b[22m for \u001b[0m\u001b[31m\u001b[1muser:hba\u001b[0m\u001b[35m\u001b[22m\u001b[K\u001b[0m\u001b[35m\u001b[22m\u001b[0m\n",
      "\u001b[32m\u001b[22mResolving card: NNFlow/7249/start/135837\u001b[K\u001b[0m\u001b[32m\u001b[22m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! python ../flows/local/NN_flow.py card view start"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:full-stack-metaflow]",
   "language": "python",
   "name": "conda-env-full-stack-metaflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
