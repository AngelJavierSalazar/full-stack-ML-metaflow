{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "923b9a08-9b88-424d-9512-4fef84a51b4f",
   "metadata": {},
   "source": [
    "## Writing Local Machine Learning Flows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f611f465-a272-434b-8331-2158cf8e9870",
   "metadata": {},
   "source": [
    "In this section, we take the machine learning scripts from the previous lesson and turn them into flows. Currently, in the spirit of not introducing more tools, we'll write our flows in notebook cells and execute them here also (this may change)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e696b923-37eb-4412-8d02-72105436a36d",
   "metadata": {},
   "source": [
    "### What is a (meta)flow?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7fb830-897a-4064-803a-eb58c6d40582",
   "metadata": {},
   "source": [
    "Include brief introduction to DAGs and use similar images to this (or find higher res version):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ba22da-ed85-4c78-9cea-fac7d704dbb8",
   "metadata": {},
   "source": [
    "![flow0](../img/flow_ex_0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f849a81d-20c8-4337-8050-b69f328cc9fe",
   "metadata": {},
   "source": [
    "Flows and DAGs can often be more complicated:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832a624d-6cd3-4dd7-b98a-d79439c9a7b7",
   "metadata": {},
   "source": [
    "![flow0](../img/flow_ex_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed0ab17-f71d-4e35-8862-346db0b1bbdb",
   "metadata": {},
   "source": [
    "So ML flows can be broken down into steps, such as:\n",
    "\n",
    "- importing data\n",
    "- processing, wrangling, and/or transforming the data\n",
    "- data validation\n",
    "- model configuration\n",
    "- model training, and\n",
    "- model deployment.\n",
    "\n",
    "The first flow we write will be a template showing these steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e25950f2-13e6-4f33-a882-8f928b952b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../flows/flow_template.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../flows/flow_template.py\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "Template for writing Metaflows\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from metaflow import FlowSpec, step, batch, current, environment, S3, card\n",
    "\n",
    "\n",
    "class Template_Flow(FlowSpec):\n",
    "    \"\"\"\n",
    "    Template for Metaflows.\n",
    "    You can choose which steps suit your workflow.\n",
    "    We have included the following common steps:\n",
    "    - Start\n",
    "    - Process data\n",
    "    - Data validation\n",
    "    - Model configuration\n",
    "    - Model training\n",
    "    - Model deployment\n",
    "    \"\"\"\n",
    "    \n",
    "    @card\n",
    "    @step\n",
    "    def start(self):\n",
    "        \"\"\"\n",
    "        Start Step for a Flow;\n",
    "        \"\"\"\n",
    "        print(\"flow name: %s\" % current.flow_name)\n",
    "        print(\"run id: %s\" % current.run_id)\n",
    "        print(\"username: %s\" % current.username)\n",
    "\n",
    "        # Call next step in DAG with self.next(...)\n",
    "        self.next(self.process_raw_data)\n",
    "\n",
    "    @step\n",
    "    def process_raw_data(self):\n",
    "        \"\"\"\n",
    "        Read and process data\n",
    "        \"\"\"\n",
    "        print(\"In this step, you'll read in and process your data\")\n",
    "\n",
    "        self.next(self.data_validation)\n",
    "\n",
    "    @step\n",
    "    def data_validation(self):\n",
    "        \"\"\"\n",
    "        Perform data validation\n",
    "        \"\"\"\n",
    "        print(\"In this step, you'll write your data validation code\")\n",
    "\n",
    "        self.next(self.get_model_config)\n",
    "\n",
    "    @step\n",
    "    def get_model_config(self):\n",
    "        \"\"\"\n",
    "        Configure model + hyperparams\n",
    "        \"\"\"\n",
    "        print(\"In this step, you'll configure your model + hyperparameters\")\n",
    "        self.next(self.train_model)\n",
    "\n",
    "    @step\n",
    "    def train_model(self):\n",
    "        \"\"\"\n",
    "        Train your model\n",
    "        \"\"\"\n",
    "        print(\"In this step, you'll train your model\")\n",
    "\n",
    "        self.next(self.deploy)\n",
    "\n",
    "    @step\n",
    "    def deploy(self):\n",
    "        \"\"\"\n",
    "        Deploy model\n",
    "        \"\"\"\n",
    "        print(\"In this step, you'll deploy your model\")\n",
    "\n",
    "        self.next(self.end)\n",
    "\n",
    "    @step\n",
    "    def end(self):\n",
    "        \"\"\"\n",
    "        DAG is done! Congrats!\n",
    "        \"\"\"\n",
    "        print('DAG ended! Woohoo!')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    Template_Flow()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8eff4783-4962-4422-8654-bb3d40abb1e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m\u001b[1mMetaflow 2.5.0\u001b[0m\u001b[35m\u001b[22m executing \u001b[0m\u001b[31m\u001b[1mTemplate_Flow\u001b[0m\u001b[35m\u001b[22m\u001b[0m\u001b[35m\u001b[22m for \u001b[0m\u001b[31m\u001b[1muser:hba\u001b[0m\u001b[35m\u001b[22m\u001b[K\u001b[0m\u001b[35m\u001b[22m\u001b[0m\n",
      "\u001b[35m\u001b[22mValidating your flow...\u001b[K\u001b[0m\u001b[35m\u001b[22m\u001b[0m\n",
      "\u001b[32m\u001b[1m    The graph looks good!\u001b[K\u001b[0m\u001b[32m\u001b[1m\u001b[0m\n",
      "\u001b[35m\u001b[22mRunning pylint...\u001b[K\u001b[0m\u001b[35m\u001b[22m\u001b[0m\n",
      "\u001b[32m\u001b[1m    Pylint is happy!\u001b[K\u001b[0m\u001b[32m\u001b[1m\u001b[0m\n",
      "\u001b[35m2022-03-16 17:44:07.233 \u001b[0m\u001b[1mWorkflow starting (run-id 7235):\u001b[0m\n",
      "\u001b[35m2022-03-16 17:44:13.159 \u001b[0m\u001b[32m[7235/start/135762 (pid 19591)] \u001b[0m\u001b[1mTask is starting.\u001b[0m\n",
      "\u001b[35m2022-03-16 17:44:20.281 \u001b[0m\u001b[32m[7235/start/135762 (pid 19591)] \u001b[0m\u001b[22mflow name: Template_Flow\u001b[0m\n",
      "\u001b[35m2022-03-16 17:44:48.697 \u001b[0m\u001b[32m[7235/start/135762 (pid 19591)] \u001b[0m\u001b[22mrun id: 7235\u001b[0m\n",
      "\u001b[35m2022-03-16 17:44:48.697 \u001b[0m\u001b[32m[7235/start/135762 (pid 19591)] \u001b[0m\u001b[22musername: hba\u001b[0m\n",
      "\u001b[35m2022-03-16 17:44:51.782 \u001b[0m\u001b[32m[7235/start/135762 (pid 19591)] \u001b[0m\u001b[1mTask finished successfully.\u001b[0m\n",
      "\u001b[35m2022-03-16 17:44:55.909 \u001b[0m\u001b[32m[7235/process_raw_data/135763 (pid 19604)] \u001b[0m\u001b[1mTask is starting.\u001b[0m\n",
      "\u001b[35m2022-03-16 17:45:03.063 \u001b[0m\u001b[32m[7235/process_raw_data/135763 (pid 19604)] \u001b[0m\u001b[22mIn this step, you'll read in and process your data\u001b[0m\n",
      "\u001b[35m2022-03-16 17:45:11.824 \u001b[0m\u001b[32m[7235/process_raw_data/135763 (pid 19604)] \u001b[0m\u001b[1mTask finished successfully.\u001b[0m\n",
      "\u001b[35m2022-03-16 17:45:15.963 \u001b[0m\u001b[32m[7235/data_validation/135764 (pid 19614)] \u001b[0m\u001b[1mTask is starting.\u001b[0m\n",
      "\u001b[35m2022-03-16 17:45:23.265 \u001b[0m\u001b[32m[7235/data_validation/135764 (pid 19614)] \u001b[0m\u001b[22mIn this step, you'll write your data validation code\u001b[0m\n",
      "\u001b[35m2022-03-16 17:45:31.956 \u001b[0m\u001b[32m[7235/data_validation/135764 (pid 19614)] \u001b[0m\u001b[1mTask finished successfully.\u001b[0m\n",
      "\u001b[35m2022-03-16 17:45:36.100 \u001b[0m\u001b[32m[7235/get_model_config/135765 (pid 19619)] \u001b[0m\u001b[1mTask is starting.\u001b[0m\n",
      "\u001b[35m2022-03-16 17:45:43.219 \u001b[0m\u001b[32m[7235/get_model_config/135765 (pid 19619)] \u001b[0m\u001b[22mIn this step, you'll configure your model + hyperparameters\u001b[0m\n",
      "\u001b[35m2022-03-16 17:45:51.966 \u001b[0m\u001b[32m[7235/get_model_config/135765 (pid 19619)] \u001b[0m\u001b[1mTask finished successfully.\u001b[0m\n",
      "\u001b[35m2022-03-16 17:45:55.995 \u001b[0m\u001b[32m[7235/train_model/135766 (pid 19626)] \u001b[0m\u001b[1mTask is starting.\u001b[0m\n",
      "\u001b[35m2022-03-16 17:46:02.986 \u001b[0m\u001b[32m[7235/train_model/135766 (pid 19626)] \u001b[0m\u001b[22mIn this step, you'll train your model\u001b[0m\n",
      "\u001b[35m2022-03-16 17:46:11.581 \u001b[0m\u001b[32m[7235/train_model/135766 (pid 19626)] \u001b[0m\u001b[1mTask finished successfully.\u001b[0m\n",
      "\u001b[35m2022-03-16 17:46:15.560 \u001b[0m\u001b[32m[7235/deploy/135768 (pid 19640)] \u001b[0m\u001b[1mTask is starting.\u001b[0m\n",
      "\u001b[35m2022-03-16 17:46:22.626 \u001b[0m\u001b[32m[7235/deploy/135768 (pid 19640)] \u001b[0m\u001b[22mIn this step, you'll deploy your model\u001b[0m\n",
      "\u001b[35m2022-03-16 17:46:31.035 \u001b[0m\u001b[32m[7235/deploy/135768 (pid 19640)] \u001b[0m\u001b[1mTask finished successfully.\u001b[0m\n",
      "\u001b[35m2022-03-16 17:46:35.239 \u001b[0m\u001b[32m[7235/end/135770 (pid 19650)] \u001b[0m\u001b[1mTask is starting.\u001b[0m\n",
      "\u001b[35m2022-03-16 17:46:42.277 \u001b[0m\u001b[32m[7235/end/135770 (pid 19650)] \u001b[0m\u001b[22mDAG ended! Woohoo!\u001b[0m\n",
      "\u001b[35m2022-03-16 17:46:50.759 \u001b[0m\u001b[32m[7235/end/135770 (pid 19650)] \u001b[0m\u001b[1mTask finished successfully.\u001b[0m\n",
      "\u001b[35m2022-03-16 17:46:51.867 \u001b[0m\u001b[1mDone!\u001b[0m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! python ../flows/flow_template.py run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727715f2-98e2-461f-9841-e019c3a66dba",
   "metadata": {},
   "source": [
    "What are all these outputs? I'm glad that you asked!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f0f3cb-4faf-452d-8a81-e457f01c286d",
   "metadata": {},
   "source": [
    "![flow0](../img/mf_output.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3594071a-8dbf-4e42-b488-f7d53cb54152",
   "metadata": {},
   "source": [
    "- **Timestamp** denotes when the line was output.\n",
    "- The information inside the square brackets identifies a **task**.\n",
    "- Every Metaflow run gets a unique ID, a **run ID**.\n",
    "- A run executes the steps in order. The step that is currently being executed is denoted by **step name**.\n",
    "- A step may spawn multiple tasks which are identified by a **task ID**.\n",
    "- The combination of a flow name, run ID, step name, and a task ID,uniquely identify a task in your Metaflow environment, amongst all runs of any flows. Here, the flow name is omitted since it is the same for all lines. We call this globally unique identifier a **pathspec**.\n",
    "- Each task is executed by a separate process in your operating system, identified by a **process ID** aka _pid_. You can use any operating system level monitoring tools such as top to monitor resource consumption of a task based on its process ID.\n",
    "- After the square bracket comes a **log message** that may be a message output by Metaflow itself, like “Task is starting” in this example, or a line output by your code.\n",
    "\n",
    "_Note_: The above bullets are almost verbatim from VT's book."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783703ae-daa1-4126-97cb-afcbc2511cd8",
   "metadata": {},
   "source": [
    "### Metaflow cards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b495bbe1-9619-4910-b693-365ebfb840e0",
   "metadata": {},
   "source": [
    "We can use MF cards to visualize aspects of our flow. In this case, there's not much to check out but we **can** see the DAG!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dadf0e77-0f7e-4aa0-a598-6aefff266f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m\u001b[1mMetaflow 2.5.0\u001b[0m\u001b[35m\u001b[22m executing \u001b[0m\u001b[31m\u001b[1mTemplate_Flow\u001b[0m\u001b[35m\u001b[22m\u001b[0m\u001b[35m\u001b[22m for \u001b[0m\u001b[31m\u001b[1muser:hba\u001b[0m\u001b[35m\u001b[22m\u001b[K\u001b[0m\u001b[35m\u001b[22m\u001b[0m\n",
      "\u001b[32m\u001b[22mResolving card: Template_Flow/7235/start/135762\u001b[K\u001b[0m\u001b[32m\u001b[22m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! python ../flows/flow_template.py card view start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b756922-edf6-44d9-8994-e4414ceb290a",
   "metadata": {},
   "source": [
    "### Random Forests ---> Metaflows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ebaf4f-7e2c-4c1c-aa62-9a045c9275e7",
   "metadata": {},
   "source": [
    "In this section, we'll turn the random forest above into a flow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5a864d3-32ad-4841-a684-66a53c3c4a95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../flows/local/rf_flow.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../flows/local/rf_flow.py\n",
    "\n",
    "from metaflow import FlowSpec, step, Parameter, JSONType, IncludeFile, card\n",
    "import json\n",
    "\n",
    "class ClassificationFlow(FlowSpec):\n",
    "    \"\"\"\n",
    "    train a random forest\n",
    "    \"\"\"\n",
    "    @card \n",
    "    @step\n",
    "    def start(self):\n",
    "        \"\"\"\n",
    "        Load the data\n",
    "        \"\"\"\n",
    "        #Import scikit-learn dataset library\n",
    "        from sklearn import datasets\n",
    "\n",
    "        #Load dataset\n",
    "        self.iris = datasets.load_iris()\n",
    "        self.X = self.iris['data']\n",
    "        self.y = self.iris['target']\n",
    "        self.next(self.rf_model)\n",
    "        \n",
    "\n",
    "    @step\n",
    "    def rf_model(self):\n",
    "        \"\"\"\n",
    "        build random forest model\n",
    "        \"\"\"\n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        \n",
    "        self.clf = RandomForestClassifier(n_estimators=10, max_depth=None,\n",
    "            min_samples_split=2, random_state=0)\n",
    "        self.next(self.train)\n",
    "\n",
    "        \n",
    "        \n",
    "    @step\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Train the model\n",
    "        \"\"\"\n",
    "        from sklearn.model_selection import cross_val_score\n",
    "        self.scores = cross_val_score(self.clf, self.X, self.y, cv=5)\n",
    "        self.next(self.end)\n",
    "        \n",
    "        \n",
    "    @step\n",
    "    def end(self):\n",
    "        \"\"\"\n",
    "        End of flow, yo!\n",
    "        \"\"\"\n",
    "        print(\"ClassificationFlow is all done.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ClassificationFlow()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7703623-d4cb-4641-81f1-19ddb43c6fb0",
   "metadata": {},
   "source": [
    "Execute the above from the command line with\n",
    "\n",
    "```bash\n",
    "python flows/local/rf_flow.py run\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37f0883b-a118-4aa6-a340-f0afde05c376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m\u001b[1mMetaflow 2.5.0\u001b[0m\u001b[35m\u001b[22m executing \u001b[0m\u001b[31m\u001b[1mClassificationFlow\u001b[0m\u001b[35m\u001b[22m\u001b[0m\u001b[35m\u001b[22m for \u001b[0m\u001b[31m\u001b[1muser:hba\u001b[0m\u001b[35m\u001b[22m\u001b[K\u001b[0m\u001b[35m\u001b[22m\u001b[0m\n",
      "\u001b[35m\u001b[22mValidating your flow...\u001b[K\u001b[0m\u001b[35m\u001b[22m\u001b[0m\n",
      "\u001b[32m\u001b[1m    The graph looks good!\u001b[K\u001b[0m\u001b[32m\u001b[1m\u001b[0m\n",
      "\u001b[35m\u001b[22mRunning pylint...\u001b[K\u001b[0m\u001b[35m\u001b[22m\u001b[0m\n",
      "\u001b[32m\u001b[1m    Pylint is happy!\u001b[K\u001b[0m\u001b[32m\u001b[1m\u001b[0m\n",
      "\u001b[35m2022-03-16 17:47:32.165 \u001b[0m\u001b[1mWorkflow starting (run-id 7237):\u001b[0m\n",
      "\u001b[35m2022-03-16 17:47:38.219 \u001b[0m\u001b[32m[7237/start/135772 (pid 19674)] \u001b[0m\u001b[1mTask is starting.\u001b[0m\n",
      "\u001b[35m2022-03-16 17:48:18.373 \u001b[0m\u001b[32m[7237/start/135772 (pid 19674)] \u001b[0m\u001b[1mTask finished successfully.\u001b[0m\n",
      "\u001b[35m2022-03-16 17:48:22.219 \u001b[0m\u001b[32m[7237/rf_model/135773 (pid 19687)] \u001b[0m\u001b[1mTask is starting.\u001b[0m\n",
      "\u001b[35m2022-03-16 17:48:39.410 \u001b[0m\u001b[32m[7237/rf_model/135773 (pid 19687)] \u001b[0m\u001b[1mTask finished successfully.\u001b[0m\n",
      "\u001b[35m2022-03-16 17:48:43.308 \u001b[0m\u001b[32m[7237/train/135774 (pid 19692)] \u001b[0m\u001b[1mTask is starting.\u001b[0m\n",
      "\u001b[35m2022-03-16 17:49:01.913 \u001b[0m\u001b[32m[7237/train/135774 (pid 19692)] \u001b[0m\u001b[1mTask finished successfully.\u001b[0m\n",
      "\u001b[35m2022-03-16 17:49:05.788 \u001b[0m\u001b[32m[7237/end/135775 (pid 19699)] \u001b[0m\u001b[1mTask is starting.\u001b[0m\n",
      "\u001b[35m2022-03-16 17:49:12.884 \u001b[0m\u001b[32m[7237/end/135775 (pid 19699)] \u001b[0m\u001b[22mClassificationFlow is all done.\u001b[0m\n",
      "\u001b[35m2022-03-16 17:49:21.532 \u001b[0m\u001b[32m[7237/end/135775 (pid 19699)] \u001b[0m\u001b[1mTask finished successfully.\u001b[0m\n",
      "\u001b[35m2022-03-16 17:49:22.750 \u001b[0m\u001b[1mDone!\u001b[0m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! python ../flows/local/rf_flow.py run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53459547-9b72-4eec-944e-71db6517faa0",
   "metadata": {},
   "source": [
    "We can check out the Metaflow card:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e89f934a-fc7f-4185-a61b-25f32dfa273c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m\u001b[1mMetaflow 2.5.0\u001b[0m\u001b[35m\u001b[22m executing \u001b[0m\u001b[31m\u001b[1mClassificationFlow\u001b[0m\u001b[35m\u001b[22m\u001b[0m\u001b[35m\u001b[22m for \u001b[0m\u001b[31m\u001b[1muser:hba\u001b[0m\u001b[35m\u001b[22m\u001b[K\u001b[0m\u001b[35m\u001b[22m\u001b[0m\n",
      "\u001b[32m\u001b[22mResolving card: ClassificationFlow/7237/start/135772\u001b[K\u001b[0m\u001b[32m\u001b[22m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! python ../flows/local/rf_flow.py card view start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde03f82-6d44-476c-99b2-1eb41e390d4f",
   "metadata": {},
   "source": [
    "Now we'll write a flow that has random forests, decision trees, and extra trees classifiers, tries them all and chooses the best one. We'll use the concept of branching, which is exemplified in this figure:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927f3fcf-3f11-4885-a68b-87adc23c016d",
   "metadata": {},
   "source": [
    "![flow0](../img/flow_ex_0.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5545fc2d-dcc4-4a37-bc8b-37ed28f98ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../flows/local/tree_branch_flow.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../flows/local/tree_branch_flow.py\n",
    "\n",
    "from metaflow import FlowSpec, step, Parameter, JSONType, IncludeFile, card\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ClassificationFlow(FlowSpec):\n",
    "    \"\"\"\n",
    "    train multiple tree based methods\n",
    "    \"\"\"\n",
    "    @card \n",
    "    @step\n",
    "    def start(self):\n",
    "        \"\"\"\n",
    "        Load the data\n",
    "        \"\"\"\n",
    "        #Import scikit-learn dataset library\n",
    "        from sklearn import datasets\n",
    "\n",
    "        #Load dataset\n",
    "        self.iris = datasets.load_iris()\n",
    "        self.X = self.iris['data']\n",
    "        self.y = self.iris['target']\n",
    "        self.next(self.rf_model, self.xt_model, self.dt_model)\n",
    "    \n",
    "                \n",
    "    @step\n",
    "    def rf_model(self):\n",
    "        \"\"\"\n",
    "        build random forest model\n",
    "        \"\"\"\n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        from sklearn.model_selection import cross_val_score\n",
    "        \n",
    "        self.clf = RandomForestClassifier(n_estimators=10, max_depth=None,\n",
    "            min_samples_split=2, random_state=0)\n",
    "        self.scores = cross_val_score(self.clf, self.X, self.y, cv=5)\n",
    "        self.next(self.choose_model)\n",
    "\n",
    "    @step\n",
    "    def xt_model(self):\n",
    "        \"\"\"\n",
    "        build extra trees classifier\n",
    "        \"\"\"\n",
    "        from sklearn.ensemble import ExtraTreesClassifier\n",
    "        from sklearn.model_selection import cross_val_score\n",
    "        \n",
    "\n",
    "        self.clf = ExtraTreesClassifier(n_estimators=10, max_depth=None,\n",
    "            min_samples_split=2, random_state=0)\n",
    "\n",
    "        self.scores = cross_val_score(self.clf, self.X, self.y, cv=5)\n",
    "        self.next(self.choose_model)\n",
    "\n",
    "    @step\n",
    "    def dt_model(self):\n",
    "        \"\"\"\n",
    "        build decision tree classifier\n",
    "        \"\"\"\n",
    "        from sklearn.tree import DecisionTreeClassifier\n",
    "        from sklearn.model_selection import cross_val_score\n",
    "        \n",
    "        self.clf = DecisionTreeClassifier(max_depth=None, min_samples_split=2,\n",
    "            random_state=0)\n",
    "\n",
    "        self.scores = cross_val_score(self.clf, self.X, self.y, cv=5)\n",
    "\n",
    "        self.next(self.choose_model)\n",
    "                        \n",
    "    @step\n",
    "    def choose_model(self, inputs):\n",
    "        \"\"\"\n",
    "        find 'best' model\n",
    "        \"\"\"\n",
    "        import numpy as np\n",
    "\n",
    "        def score(inp):\n",
    "            return inp.clf,\\\n",
    "                   np.mean(inp.scores)\n",
    "\n",
    "            \n",
    "        self.results = sorted(map(score, inputs), key=lambda x: -x[1]) \n",
    "        self.model = self.results[0][0]\n",
    "        self.next(self.end)\n",
    "        \n",
    "    @step\n",
    "    def end(self):\n",
    "        \"\"\"\n",
    "        End of flow, yo!\n",
    "        \"\"\"\n",
    "        print('Scores:')\n",
    "        print('\\n'.join('%s %f' % res for res in self.results))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ClassificationFlow()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6fc354-9ef2-4326-a913-f2299fd3d8b4",
   "metadata": {},
   "source": [
    "Execute the above from the command line with\n",
    "\n",
    "```bash\n",
    "python flows/local/tree_branch_flow.py run\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3aa5a415-63b2-412f-a625-c7ccaeb30f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m\u001b[1mMetaflow 2.5.0\u001b[0m\u001b[35m\u001b[22m executing \u001b[0m\u001b[31m\u001b[1mClassificationFlow\u001b[0m\u001b[35m\u001b[22m\u001b[0m\u001b[35m\u001b[22m for \u001b[0m\u001b[31m\u001b[1muser:hba\u001b[0m\u001b[35m\u001b[22m\u001b[K\u001b[0m\u001b[35m\u001b[22m\u001b[0m\n",
      "\u001b[35m\u001b[22mValidating your flow...\u001b[K\u001b[0m\u001b[35m\u001b[22m\u001b[0m\n",
      "\u001b[32m\u001b[1m    The graph looks good!\u001b[K\u001b[0m\u001b[32m\u001b[1m\u001b[0m\n",
      "\u001b[35m\u001b[22mRunning pylint...\u001b[K\u001b[0m\u001b[35m\u001b[22m\u001b[0m\n",
      "\u001b[32m\u001b[1m    Pylint is happy!\u001b[K\u001b[0m\u001b[32m\u001b[1m\u001b[0m\n",
      "\u001b[35m2022-03-16 17:51:38.796 \u001b[0m\u001b[1mWorkflow starting (run-id 7238):\u001b[0m\n",
      "\u001b[35m2022-03-16 17:51:44.758 \u001b[0m\u001b[32m[7238/start/135778 (pid 19734)] \u001b[0m\u001b[1mTask is starting.\u001b[0m\n",
      "\u001b[35m2022-03-16 17:52:24.645 \u001b[0m\u001b[32m[7238/start/135778 (pid 19734)] \u001b[0m\u001b[1mTask finished successfully.\u001b[0m\n",
      "\u001b[35m2022-03-16 17:52:28.484 \u001b[0m\u001b[32m[7238/rf_model/135780 (pid 19751)] \u001b[0m\u001b[1mTask is starting.\u001b[0m\n",
      "\u001b[35m2022-03-16 17:52:31.303 \u001b[0m\u001b[32m[7238/xt_model/135781 (pid 19755)] \u001b[0m\u001b[1mTask is starting.\u001b[0m\n",
      "\u001b[35m2022-03-16 17:52:34.573 \u001b[0m\u001b[32m[7238/dt_model/135782 (pid 19759)] \u001b[0m\u001b[1mTask is starting.\u001b[0m\n",
      "\u001b[35m2022-03-16 17:52:47.416 \u001b[0m\u001b[32m[7238/rf_model/135780 (pid 19751)] \u001b[0m\u001b[1mTask finished successfully.\u001b[0m\n",
      "\u001b[35m2022-03-16 17:52:50.889 \u001b[0m\u001b[32m[7238/xt_model/135781 (pid 19755)] \u001b[0m\u001b[1mTask finished successfully.\u001b[0m\n",
      "\u001b[35m2022-03-16 17:52:54.306 \u001b[0m\u001b[32m[7238/dt_model/135782 (pid 19759)] \u001b[0m\u001b[1mTask finished successfully.\u001b[0m\n",
      "\u001b[35m2022-03-16 17:52:58.281 \u001b[0m\u001b[32m[7238/choose_model/135783 (pid 19772)] \u001b[0m\u001b[1mTask is starting.\u001b[0m\n",
      "\u001b[35m2022-03-16 17:53:23.490 \u001b[0m\u001b[32m[7238/choose_model/135783 (pid 19772)] \u001b[0m\u001b[1mTask finished successfully.\u001b[0m\n",
      "\u001b[35m2022-03-16 17:53:27.320 \u001b[0m\u001b[32m[7238/end/135785 (pid 19784)] \u001b[0m\u001b[1mTask is starting.\u001b[0m\n",
      "\u001b[35m2022-03-16 17:53:34.981 \u001b[0m\u001b[32m[7238/end/135785 (pid 19784)] \u001b[0m\u001b[22mScores:\u001b[0m\n",
      "\u001b[35m2022-03-16 17:53:35.594 \u001b[0m\u001b[32m[7238/end/135785 (pid 19784)] \u001b[0m\u001b[22mDecisionTreeClassifier(random_state=0) 0.960000\u001b[0m\n",
      "\u001b[35m2022-03-16 17:53:41.401 \u001b[0m\u001b[32m[7238/end/135785 (pid 19784)] \u001b[0m\u001b[22mRandomForestClassifier(n_estimators=10, random_state=0) 0.953333\u001b[0m\n",
      "\u001b[35m2022-03-16 17:53:41.401 \u001b[0m\u001b[32m[7238/end/135785 (pid 19784)] \u001b[0m\u001b[22mExtraTreesClassifier(n_estimators=10, random_state=0) 0.953333\u001b[0m\n",
      "\u001b[35m2022-03-16 17:53:44.282 \u001b[0m\u001b[32m[7238/end/135785 (pid 19784)] \u001b[0m\u001b[1mTask finished successfully.\u001b[0m\n",
      "\u001b[35m2022-03-16 17:53:45.408 \u001b[0m\u001b[1mDone!\u001b[0m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! python ../flows/local/tree_branch_flow.py run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3498134b-2f69-4dd2-b604-6e9c6e44fd49",
   "metadata": {},
   "source": [
    "We can also view the Metaflow card:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a321df46-1ba8-4035-add6-a81d5ebe23d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m\u001b[1mMetaflow 2.5.0\u001b[0m\u001b[35m\u001b[22m executing \u001b[0m\u001b[31m\u001b[1mClassificationFlow\u001b[0m\u001b[35m\u001b[22m\u001b[0m\u001b[35m\u001b[22m for \u001b[0m\u001b[31m\u001b[1muser:hba\u001b[0m\u001b[35m\u001b[22m\u001b[K\u001b[0m\u001b[35m\u001b[22m\u001b[0m\n",
      "\u001b[32m\u001b[22mResolving card: ClassificationFlow/7238/start/135778\u001b[K\u001b[0m\u001b[32m\u001b[22m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! python ../flows/local/tree_branch_flow.py card view start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2dd4a43-3ea3-43f5-9d2f-c96ee7d18bb3",
   "metadata": {},
   "source": [
    "### Boosted Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37b27a2-f26c-471d-8164-598ed7d4cfb0",
   "metadata": {},
   "source": [
    "In this section, we'll turn the xgboost example above into a flow.\n",
    "\n",
    "_Note:_ you've done this _almost_ trivially below, really to show yourself that you can get xgboost working with MF, HBA. Make a slightly stronger example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6e0b86aa-498d-474f-bde4-1e183b5dd7a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../flows/local/boosted_flow.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../flows/local/boosted_flow.py\n",
    "\n",
    "from metaflow import FlowSpec, step, Parameter, JSONType, IncludeFile, card\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class BSTFlow(FlowSpec):\n",
    "    \"\"\"\n",
    "    train a boosted tree\n",
    "    \"\"\"\n",
    "    @card\n",
    "    @step\n",
    "    def start(self):\n",
    "        \"\"\"\n",
    "        Load the data & train model\n",
    "        \"\"\"\n",
    "        import xgboost as xgb\n",
    "        # from io import StringIO\n",
    "        # read in data\n",
    "        dtrain = xgb.DMatrix('../data/agaricus.txt.train')\n",
    "        #dtest = xgb.DMatrix('data/agaricus.txt.test')\n",
    "\n",
    "                # specify parameters\n",
    "        param = {'max_depth':2, 'eta':1, 'objective':'binary:logistic' }\n",
    "        num_round = 2\n",
    "        bst = xgb.train(param, dtrain, num_round)\n",
    "        bst.save_model(\"model.json\")\n",
    "        self.next(self.predict)\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "    @step\n",
    "    def predict(self):\n",
    "        \"\"\"\n",
    "        make predictions\n",
    "        \"\"\"\n",
    "        import xgboost as xgb\n",
    "\n",
    "        dtest = xgb.DMatrix('../data/agaricus.txt.test')\n",
    "        # make prediction\n",
    "        bst = xgb.Booster()\n",
    "        bst.load_model(\"model.json\")\n",
    "        preds = bst.predict(dtest)\n",
    "        self.next(self.end)\n",
    "        \n",
    "        \n",
    "    @step\n",
    "    def end(self):\n",
    "        \"\"\"\n",
    "        End of flow, yo!\n",
    "        \"\"\"\n",
    "        print(\"ClassificationFlow is all done.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    BSTFlow()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a1fce4-47cd-4c23-b770-7af10f785aee",
   "metadata": {},
   "source": [
    "Execute the above from the command line with\n",
    "\n",
    "```bash\n",
    "python flows/local/boosted_flow.py run\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2e817331-98cf-4eca-bea4-036fc41965c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m\u001b[1mMetaflow 2.5.0\u001b[0m\u001b[35m\u001b[22m executing \u001b[0m\u001b[31m\u001b[1mBSTFlow\u001b[0m\u001b[35m\u001b[22m\u001b[0m\u001b[35m\u001b[22m for \u001b[0m\u001b[31m\u001b[1muser:hba\u001b[0m\u001b[35m\u001b[22m\u001b[K\u001b[0m\u001b[35m\u001b[22m\u001b[0m\n",
      "\u001b[35m\u001b[22mValidating your flow...\u001b[K\u001b[0m\u001b[35m\u001b[22m\u001b[0m\n",
      "\u001b[32m\u001b[1m    The graph looks good!\u001b[K\u001b[0m\u001b[32m\u001b[1m\u001b[0m\n",
      "\u001b[35m\u001b[22mRunning pylint...\u001b[K\u001b[0m\u001b[35m\u001b[22m\u001b[0m\n",
      "\u001b[32m\u001b[1m    Pylint is happy!\u001b[K\u001b[0m\u001b[32m\u001b[1m\u001b[0m\n",
      "\u001b[35m2022-03-16 17:57:34.426 \u001b[0m\u001b[1mWorkflow starting (run-id 7241):\u001b[0m\n",
      "\u001b[35m2022-03-16 17:57:40.317 \u001b[0m\u001b[32m[7241/start/135791 (pid 19867)] \u001b[0m\u001b[1mTask is starting.\u001b[0m\n",
      "\u001b[35m2022-03-16 17:57:47.953 \u001b[0m\u001b[32m[7241/start/135791 (pid 19867)] \u001b[0m\u001b[22m[17:57:47] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\u001b[0m\n",
      "\u001b[35m2022-03-16 17:58:19.118 \u001b[0m\u001b[32m[7241/start/135791 (pid 19867)] \u001b[0m\u001b[1mTask finished successfully.\u001b[0m\n",
      "\u001b[35m2022-03-16 17:58:23.216 \u001b[0m\u001b[32m[7241/predict/135792 (pid 19877)] \u001b[0m\u001b[1mTask is starting.\u001b[0m\n",
      "\u001b[35m2022-03-16 17:58:39.779 \u001b[0m\u001b[32m[7241/predict/135792 (pid 19877)] \u001b[0m\u001b[1mTask finished successfully.\u001b[0m\n",
      "\u001b[35m2022-03-16 17:58:43.670 \u001b[0m\u001b[32m[7241/end/135793 (pid 19883)] \u001b[0m\u001b[1mTask is starting.\u001b[0m\n",
      "\u001b[35m2022-03-16 17:58:50.741 \u001b[0m\u001b[32m[7241/end/135793 (pid 19883)] \u001b[0m\u001b[22mClassificationFlow is all done.\u001b[0m\n",
      "\u001b[35m2022-03-16 17:58:59.514 \u001b[0m\u001b[32m[7241/end/135793 (pid 19883)] \u001b[0m\u001b[1mTask finished successfully.\u001b[0m\n",
      "\u001b[35m2022-03-16 17:59:00.776 \u001b[0m\u001b[1mDone!\u001b[0m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! python ../flows/local/boosted_flow.py run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074ee36d-c8d1-4d99-9f09-999324974ccd",
   "metadata": {},
   "source": [
    "We can also view the Metaflow card:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e4b1d4a9-ad3e-4043-a011-6229b92e1fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m\u001b[1mMetaflow 2.5.0\u001b[0m\u001b[35m\u001b[22m executing \u001b[0m\u001b[31m\u001b[1mBSTFlow\u001b[0m\u001b[35m\u001b[22m\u001b[0m\u001b[35m\u001b[22m for \u001b[0m\u001b[31m\u001b[1muser:hba\u001b[0m\u001b[35m\u001b[22m\u001b[K\u001b[0m\u001b[35m\u001b[22m\u001b[0m\n",
      "\u001b[32m\u001b[22mResolving card: BSTFlow/7241/start/135791\u001b[K\u001b[0m\u001b[32m\u001b[22m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! python ../flows/local/boosted_flow.py card view start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f757332f-b475-4e42-b546-c97a75068fc9",
   "metadata": {},
   "source": [
    "### Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e95df17-f187-4d4f-ad2e-894898f9b9b0",
   "metadata": {},
   "source": [
    "In this section, we'll turn the deep learning example above into a flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "92adeacf-5396-42d0-bed2-32a8a9763d1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../flows/local/NN_flow.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../flows/local/NN_flow.py\n",
    "\n",
    "from metaflow import FlowSpec, step, Parameter, JSONType, IncludeFile, card\n",
    "from taxi_modules import init, MODELS, MODEL_LIBRARIES\n",
    "import json\n",
    "\n",
    "\n",
    "class NNFlow(FlowSpec):\n",
    "    \"\"\"\n",
    "    train a NN\n",
    "    \"\"\"\n",
    "    @card\n",
    "    @step\n",
    "    def start(self):\n",
    "        \"\"\"\n",
    "        Load the data\n",
    "        \"\"\"\n",
    "        from tensorflow import keras\n",
    "\n",
    "        # the data, split between train and test sets\n",
    "        (self.x_train, self.y_train), (self.x_test, self.y_test) = keras.datasets.mnist.load_data()\n",
    "        self.next(self.wrangle)\n",
    "        \n",
    "    @step\n",
    "    def wrangle(self):\n",
    "        \"\"\"\n",
    "        massage data\n",
    "        \"\"\"\n",
    "        import numpy as np\n",
    "        from tensorflow import keras\n",
    "        # Model / data parameters\n",
    "        self.num_classes = 10\n",
    "        self.input_shape = (28, 28, 1)\n",
    "\n",
    "        # Scale images to the [0, 1] range\n",
    "        self.x_train = self.x_train.astype(\"float32\") / 255\n",
    "        self.x_test = self.x_test.astype(\"float32\") / 255\n",
    "        # Make sure images have shape (28, 28, 1)\n",
    "        self.x_train = np.expand_dims(self.x_train, -1)\n",
    "        self.x_test = np.expand_dims(self.x_test, -1)\n",
    "\n",
    "        # convert class vectors to binary class matrices\n",
    "        self.y_train = keras.utils.to_categorical(self.y_train, self.num_classes)\n",
    "        self.y_test = keras.utils.to_categorical(self.y_test, self.num_classes)\n",
    "        \n",
    "        self.next(self.build_model)\n",
    "\n",
    "\n",
    "    @step\n",
    "    def build_model(self):\n",
    "        \"\"\"\n",
    "        build NN model\n",
    "        \"\"\"\n",
    "        import tempfile\n",
    "        import numpy as np\n",
    "        import tensorflow as tf\n",
    "        from tensorflow import keras\n",
    "        from tensorflow.keras import layers\n",
    "\n",
    "        model = keras.Sequential(\n",
    "            [\n",
    "                keras.Input(shape=self.input_shape),\n",
    "                layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "                layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "                layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "                layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "                layers.Flatten(),\n",
    "                layers.Dropout(0.5),\n",
    "                layers.Dense(self.num_classes, activation=\"softmax\"),\n",
    "            ]\n",
    "        )\n",
    "        model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "        with tempfile.NamedTemporaryFile() as f:\n",
    "            tf.keras.models.save_model(model, f.name, save_format='h5')\n",
    "            self.model = f.read()\n",
    "        self.next(self.train)\n",
    "\n",
    "        \n",
    "        \n",
    "    @step\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Train the model\n",
    "        \"\"\"\n",
    "        import tempfile\n",
    "        import tensorflow as tf\n",
    "        self.batch_size = 128\n",
    "        self.epochs = 15\n",
    "        \n",
    "        with tempfile.NamedTemporaryFile() as f:\n",
    "            f.write(self.model)\n",
    "            f.flush()\n",
    "            model =  tf.keras.models.load_model(f.name)\n",
    "        model.fit(self.x_train, self.y_train, batch_size=self.batch_size, epochs=self.epochs, validation_split=0.1)\n",
    "        \n",
    "        self.next(self.end)\n",
    "        \n",
    "        \n",
    "    @step\n",
    "    def end(self):\n",
    "        \"\"\"\n",
    "        End of flow, yo!\n",
    "        \"\"\"\n",
    "        print(\"ClassificationFlow is all done.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    NNFlow()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bf1b5f-d6b9-4b89-be19-b3d22a5db0bd",
   "metadata": {},
   "source": [
    "Execute the above from the command line with\n",
    "\n",
    "```bash\n",
    "python flows/local/NN_flow.py run\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6aa50054-916d-4b5c-8f5b-4159996d95a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m\u001b[1mMetaflow 2.5.0\u001b[0m\u001b[35m\u001b[22m executing \u001b[0m\u001b[31m\u001b[1mNNFlow\u001b[0m\u001b[35m\u001b[22m\u001b[0m\u001b[35m\u001b[22m for \u001b[0m\u001b[31m\u001b[1muser:hba\u001b[0m\u001b[35m\u001b[22m\u001b[K\u001b[0m\u001b[35m\u001b[22m\u001b[0m\n",
      "\u001b[35m\u001b[22mValidating your flow...\u001b[K\u001b[0m\u001b[35m\u001b[22m\u001b[0m\n",
      "\u001b[32m\u001b[1m    The graph looks good!\u001b[K\u001b[0m\u001b[32m\u001b[1m\u001b[0m\n",
      "\u001b[35m\u001b[22mRunning pylint...\u001b[K\u001b[0m\u001b[35m\u001b[22m\u001b[0m\n",
      "\u001b[32m\u001b[1m    Pylint is happy!\u001b[K\u001b[0m\u001b[32m\u001b[1m\u001b[0m\n",
      "\u001b[35m2022-03-16 18:00:02.732 \u001b[0m\u001b[1mWorkflow starting (run-id 7242):\u001b[0m\n",
      "\u001b[35m2022-03-16 18:00:08.649 \u001b[0m\u001b[32m[7242/start/135795 (pid 19920)] \u001b[0m\u001b[1mTask is starting.\u001b[0m\n",
      "\u001b[35m2022-03-16 18:01:00.526 \u001b[0m\u001b[32m[7242/start/135795 (pid 19920)] \u001b[0m\u001b[1mTask finished successfully.\u001b[0m\n",
      "\u001b[35m2022-03-16 18:01:04.483 \u001b[0m\u001b[32m[7242/wrangle/135796 (pid 20002)] \u001b[0m\u001b[1mTask is starting.\u001b[0m\n",
      "\u001b[35m2022-03-16 18:01:31.343 \u001b[0m\u001b[32m[7242/wrangle/135796 (pid 20002)] \u001b[0m\u001b[1mTask finished successfully.\u001b[0m\n",
      "\u001b[35m2022-03-16 18:01:35.159 \u001b[0m\u001b[32m[7242/build_model/135800 (pid 20030)] \u001b[0m\u001b[1mTask is starting.\u001b[0m\n",
      "\u001b[35m2022-03-16 18:01:44.362 \u001b[0m\u001b[32m[7242/build_model/135800 (pid 20030)] \u001b[0m\u001b[22m2022-03-16 18:01:44.361807: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\u001b[0m\n",
      "\u001b[35m2022-03-16 18:01:55.963 \u001b[0m\u001b[32m[7242/build_model/135800 (pid 20030)] \u001b[0m\u001b[22mTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\u001b[0m\n",
      "\u001b[35m2022-03-16 18:02:00.042 \u001b[0m\u001b[32m[7242/build_model/135800 (pid 20030)] \u001b[0m\u001b[1mTask finished successfully.\u001b[0m\n",
      "\u001b[35m2022-03-16 18:02:04.160 \u001b[0m\u001b[32m[7242/train/135801 (pid 20069)] \u001b[0m\u001b[1mTask is starting.\u001b[0m\n",
      "\u001b[35m2022-03-16 18:02:13.257 \u001b[0m\u001b[32m[7242/train/135801 (pid 20069)] \u001b[0m\u001b[22m2022-03-16 18:02:13.257571: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\u001b[0m\n",
      "\u001b[35m2022-03-16 18:02:28.442 \u001b[0m\u001b[32m[7242/train/135801 (pid 20069)] \u001b[0m\u001b[22mTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\u001b[0m\n",
      "\u001b[35m2022-03-16 18:02:28.442 \u001b[0m\u001b[32m[7242/train/135801 (pid 20069)] \u001b[0m\u001b[22m2022-03-16 18:02:28.441918: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\u001b[0m\n",
      "\u001b[35m2022-03-16 18:02:28.454 \u001b[0m\u001b[32m[7242/train/135801 (pid 20069)] \u001b[0m\u001b[22mEpoch 1/15\u001b[0m\n",
      "422/422 [==============================] - 11s 26ms/step - loss: 0.3687 - accuracy: 0.8876 - val_loss: 0.0829 - val_accuracy: 0.9787\u001b[0m: 2.3265 - accuracy: 0.11\n",
      "\u001b[35m2022-03-16 18:02:39.846 \u001b[0m\u001b[32m[7242/train/135801 (pid 20069)] \u001b[0m\u001b[22mEpoch 2/15\u001b[0m\n",
      "422/422 [==============================] - 11s 25ms/step - loss: 0.1091 - accuracy: 0.9661 - val_loss: 0.0561 - val_accuracy: 0.9845\u001b[0m 0.1622 - accuracy: 0.937\n",
      "\u001b[35m2022-03-16 18:02:50.535 \u001b[0m\u001b[32m[7242/train/135801 (pid 20069)] \u001b[0m\u001b[22mEpoch 3/15\u001b[0m\n",
      "422/422 [==============================] - 11s 26ms/step - loss: 0.0820 - accuracy: 0.9752 - val_loss: 0.0496 - val_accuracy: 0.9865\u001b[0m0.0649 - accuracy: 0.98\n",
      "\u001b[35m2022-03-16 18:03:01.321 \u001b[0m\u001b[32m[7242/train/135801 (pid 20069)] \u001b[0m\u001b[22mEpoch 4/15\u001b[0m\n",
      "422/422 [==============================] - 11s 25ms/step - loss: 0.0707 - accuracy: 0.9782 - val_loss: 0.0431 - val_accuracy: 0.9887\u001b[0m0.0652 - accuracy: 0.98\n",
      "\u001b[35m2022-03-16 18:03:11.899 \u001b[0m\u001b[32m[7242/train/135801 (pid 20069)] \u001b[0m\u001b[22mEpoch 5/15\u001b[0m\n",
      "422/422 [==============================] - 11s 25ms/step - loss: 0.0610 - accuracy: 0.9809 - val_loss: 0.0395 - val_accuracy: 0.9905\u001b[0m 0.0110 - accuracy: 1.00\n",
      "\u001b[35m2022-03-16 18:03:22.580 \u001b[0m\u001b[32m[7242/train/135801 (pid 20069)] \u001b[0m\u001b[22mEpoch 6/15\u001b[0m\n",
      "422/422 [==============================] - 11s 25ms/step - loss: 0.0545 - accuracy: 0.9828 - val_loss: 0.0351 - val_accuracy: 0.9887\u001b[0m 0.0825 - accuracy: 0.976\n",
      "\u001b[35m2022-03-16 18:03:33.145 \u001b[0m\u001b[32m[7242/train/135801 (pid 20069)] \u001b[0m\u001b[22mEpoch 7/15\u001b[0m\n",
      "422/422 [==============================] - 11s 26ms/step - loss: 0.0500 - accuracy: 0.9843 - val_loss: 0.0367 - val_accuracy: 0.9908\u001b[0m 0.0517 - accuracy: 0.984\n",
      "\u001b[35m2022-03-16 18:03:44.099 \u001b[0m\u001b[32m[7242/train/135801 (pid 20069)] \u001b[0m\u001b[22mEpoch 8/15\u001b[0m\n",
      "422/422 [==============================] - 11s 26ms/step - loss: 0.0471 - accuracy: 0.9849 - val_loss: 0.0351 - val_accuracy: 0.9907\u001b[0m 0.0118 - accuracy: 1.000\n",
      "\u001b[35m2022-03-16 18:03:54.862 \u001b[0m\u001b[32m[7242/train/135801 (pid 20069)] \u001b[0m\u001b[22mEpoch 9/15\u001b[0m\n",
      "422/422 [==============================] - 12s 27ms/step - loss: 0.0440 - accuracy: 0.9858 - val_loss: 0.0331 - val_accuracy: 0.9908\u001b[0m0.0221 - accuracy: 1.00\n",
      "\u001b[35m2022-03-16 18:04:06.409 \u001b[0m\u001b[32m[7242/train/135801 (pid 20069)] \u001b[0m\u001b[22mEpoch 10/15\u001b[0m\n",
      "422/422 [==============================] - 11s 26ms/step - loss: 0.0399 - accuracy: 0.9871 - val_loss: 0.0322 - val_accuracy: 0.9923\u001b[0m0.0340 - accuracy: 0.98\n",
      "\u001b[35m2022-03-16 18:04:17.270 \u001b[0m\u001b[32m[7242/train/135801 (pid 20069)] \u001b[0m\u001b[22mEpoch 11/15\u001b[0m\n",
      "422/422 [==============================] - 10s 25ms/step - loss: 0.0384 - accuracy: 0.9881 - val_loss: 0.0304 - val_accuracy: 0.9918\u001b[0m 0.0407 - accuracy: 0.984\n",
      "\u001b[35m2022-03-16 18:04:27.728 \u001b[0m\u001b[32m[7242/train/135801 (pid 20069)] \u001b[0m\u001b[22mEpoch 12/15\u001b[0m\n",
      "422/422 [==============================] - 11s 26ms/step - loss: 0.0369 - accuracy: 0.9884 - val_loss: 0.0301 - val_accuracy: 0.9923\u001b[0m 0.0122 - accuracy: 1.000\n",
      "\u001b[35m2022-03-16 18:04:38.812 \u001b[0m\u001b[32m[7242/train/135801 (pid 20069)] \u001b[0m\u001b[22mEpoch 13/15\u001b[0m\n",
      "422/422 [==============================] - 11s 26ms/step - loss: 0.0356 - accuracy: 0.9885 - val_loss: 0.0316 - val_accuracy: 0.9908\u001b[0m0.0327 - accuracy: 0.99\n",
      "\u001b[35m2022-03-16 18:04:49.621 \u001b[0m\u001b[32m[7242/train/135801 (pid 20069)] \u001b[0m\u001b[22mEpoch 14/15\u001b[0m\n",
      "422/422 [==============================] - 11s 25ms/step - loss: 0.0337 - accuracy: 0.9891 - val_loss: 0.0302 - val_accuracy: 0.9918\u001b[0m 0.0440 - accuracy: 0.976\n",
      "\u001b[35m2022-03-16 18:05:00.286 \u001b[0m\u001b[32m[7242/train/135801 (pid 20069)] \u001b[0m\u001b[22mEpoch 15/15\u001b[0m\n",
      "422/422 [==============================] - 11s 26ms/step - loss: 0.0320 - accuracy: 0.9892 - val_loss: 0.0281 - val_accuracy: 0.9915\u001b[0m0.0183 - accuracy: 1.00\n",
      "\u001b[35m2022-03-16 18:05:23.460 \u001b[0m\u001b[32m[7242/train/135801 (pid 20069)] \u001b[0m\u001b[1mTask finished successfully.\u001b[0m\n",
      "\u001b[35m2022-03-16 18:05:30.366 \u001b[0m\u001b[32m[7242/end/135803 (pid 20119)] \u001b[0m\u001b[1mTask is starting.\u001b[0m\n",
      "\u001b[35m2022-03-16 18:05:37.660 \u001b[0m\u001b[32m[7242/end/135803 (pid 20119)] \u001b[0m\u001b[22mClassificationFlow is all done.\u001b[0m\n",
      "\u001b[35m2022-03-16 18:05:46.368 \u001b[0m\u001b[32m[7242/end/135803 (pid 20119)] \u001b[0m\u001b[1mTask finished successfully.\u001b[0m\n",
      "\u001b[35m2022-03-16 18:05:47.520 \u001b[0m\u001b[1mDone!\u001b[0m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! python ../flows/local/NN_flow.py run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9d4468-2e0b-4110-9cad-c7ee2ce1a55c",
   "metadata": {},
   "source": [
    "We can also view the Metaflow card:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "676c337b-d846-4cf9-96ee-038f9eada440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m\u001b[1mMetaflow 2.5.0\u001b[0m\u001b[35m\u001b[22m executing \u001b[0m\u001b[31m\u001b[1mNNFlow\u001b[0m\u001b[35m\u001b[22m\u001b[0m\u001b[35m\u001b[22m for \u001b[0m\u001b[31m\u001b[1muser:hba\u001b[0m\u001b[35m\u001b[22m\u001b[K\u001b[0m\u001b[35m\u001b[22m\u001b[0m\n",
      "\u001b[32m\u001b[22mResolving card: NNFlow/7242/start/135795\u001b[K\u001b[0m\u001b[32m\u001b[22m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! python ../flows/local/NN_flow.py card view start"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:full-stack-metaflow]",
   "language": "python",
   "name": "conda-env-full-stack-metaflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
